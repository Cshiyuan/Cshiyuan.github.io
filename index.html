<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>CShyiuan博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="miehaha">
<meta property="og:type" content="website">
<meta property="og:title" content="CShyiuan博客">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="CShyiuan博客">
<meta property="og:description" content="miehaha">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="[object Object]">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="CShyiuan博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.2.0"></head>
  <!-- highlight.js代码高亮主题 css 引入-->
  <link rel="stylesheet" href="/plugins/highlight/styles/monokai.css">
  <!-- highlight.js代码高亮主题 css 引入-->
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">CShyiuan博客</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/05/25/hello-world/" class="article-date">
  <time datetime="2022-05-25T13:52:27.000Z" itemprop="datePublished">2022-05-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/05/25/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start">Quick Start</h2>
<h3 id="Create-a-new-post">Create a new post</h3>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server">Run server</h3>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files">Generate static files</h3>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/05/25/hello-world/" data-id="cl3jmf7n6004p9jv3hbizd70b" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Hexo快速搭建" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/05/24/Hexo%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA/" class="article-date">
  <time datetime="2022-05-24T02:45:43.000Z" itemprop="datePublished">2022-05-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/05/24/Hexo%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA/">Hexo快速搭建</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="快速简单搭建Hexo">快速简单搭建Hexo</h2>
<h4 id="Homebrew-安装-node-js">Homebrew 安装 node.js</h4>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install node</span><br></pre></td></tr></table></figure>
<h4 id="安装后可以使用命令来检查是否安装成功">安装后可以使用命令来检查是否安装成功</h4>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node -v</span><br></pre></td></tr></table></figure>
<h4 id="使用-npm-安装-hexo">使用 npm 安装 hexo</h4>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install -g hexo-cli</span><br></pre></td></tr></table></figure>
<h4 id="附-hexo常用命令">附 hexo常用命令</h4>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hexo n <span class="string">&quot;博客名称&quot;</span>  =&gt; hexo new <span class="string">&quot;博客名称&quot;</span>   <span class="comment">#这两个都是创建新文章，前者是简写模式</span></span><br><span class="line">hexo p  =&gt; hexo publish</span><br><span class="line">hexo g  =&gt; hexo generate  <span class="comment">#生成</span></span><br><span class="line">hexo s  =&gt; hexo server  <span class="comment">#启动服务预览</span></span><br><span class="line">hexo d  =&gt; hexo deploy  <span class="comment">#部署</span></span><br><span class="line"></span><br><span class="line">hexo server   <span class="comment">#Hexo 会监视文件变动并自动更新，无须重启服务器。</span></span><br><span class="line">hexo server -s   <span class="comment">#静态模式</span></span><br><span class="line">hexo server -p 5000   <span class="comment">#更改端口</span></span><br><span class="line">hexo server -i 192.168.1.1   <span class="comment">#自定义IP</span></span><br><span class="line">hexo clean   <span class="comment">#清除缓存，网页正常情况下可以忽略此条命令</span></span><br><span class="line">hexo g   <span class="comment">#生成静态网页</span></span><br><span class="line">hexo d   <span class="comment">#开始部署</span></span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/05/24/Hexo%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA/" data-id="cl3jmf7mf001m9jv3fdcb3l68" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Nginx-Mysql-PM2-NODE-GIT-HTTPS开发日记" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/02/13/Nginx-Mysql-PM2-NODE-GIT-HTTPS%E5%BC%80%E5%8F%91%E6%97%A5%E8%AE%B0/" class="article-date">
  <time datetime="2018-02-13T15:10:28.000Z" itemprop="datePublished">2018-02-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/13/Nginx-Mysql-PM2-NODE-GIT-HTTPS%E5%BC%80%E5%8F%91%E6%97%A5%E8%AE%B0/">Nginx+Mysql+PM2+NODE+GIT+HTTPS开发日记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="MySql安装">MySql安装</h2>
<h4 id="下载-mysql-源安装包">下载 mysql 源安装包</h4>
<p>下载到本地</p>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl -LO http://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm</span><br></pre></td></tr></table></figure>
<h4 id="安装-mysql-源">安装 mysql 源</h4>
<p>本地安装源</p>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum localinstall mysql57-community-release-el7-11.noarch.rpm</span><br></pre></td></tr></table></figure>
<h4 id="检查-yum-源是否安装成功">检查 yum 源是否安装成功</h4>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum repolist enabled | grep <span class="string">&quot;mysql.*-community.*&quot;</span></span><br><span class="line">mysql-connectors-community           MySQL Connectors Community              21</span><br><span class="line">mysql-tools-community                MySQL Tools Community                   38</span><br><span class="line">mysql57-community                    MySQL 5.7 Community Server             130</span><br></pre></td></tr></table></figure>
<h4 id="开始安装">开始安装</h4>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install mysql-community-server</span><br></pre></td></tr></table></figure>
<h4 id="启动">启动</h4>
<ul>
<li>安装服务</li>
</ul>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl <span class="built_in">enable</span> mysqld</span><br></pre></td></tr></table></figure>
<ul>
<li>启动服务</li>
</ul>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl start mysqld</span><br></pre></td></tr></table></figure>
<ul>
<li>查看服务状态</li>
</ul>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl status mysqld</span><br></pre></td></tr></table></figure>
<h4 id="修改-root-默认密码">修改 root 默认密码</h4>
<p>MySQL 5.7 启动后，在 /var/log/mysqld.log 文件中给 root 生成了一个默认密码。通过下面的方式找到 root 默认密码，然后登录 mysql 进行修改：</p>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ grep <span class="string">&#x27;temporary password&#x27;</span> /var/log/mysqld.log</span><br><span class="line">[Note] A temporary password is generated <span class="keyword">for</span> root@localhost: **********</span><br></pre></td></tr></table></figure>
<p>登录 MySQL 并修改密码</p>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mysql -u root -p</span><br><span class="line">Enter password: </span><br><span class="line">mysql&gt; ALTER USER <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> IDENTIFIED BY <span class="string">&#x27;MyNewPass4!&#x27;</span>;</span><br></pre></td></tr></table></figure>
<h4 id="添加远程登录用户">添加远程登录用户</h4>
<p>MySQL 默认只允许 root 帐户在本地登录，如果要在其它机器上连接 MySQL，必须修改 root 允许远程连接，或者添加一个允许远程连接的帐户，为了安全起见，本例添加一个新的帐户：</p>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON *.* TO <span class="string">&#x27;admin&#x27;</span>@<span class="string">&#x27;%&#x27;</span> IDENTIFIED BY <span class="string">&#x27;secret&#x27;</span> WITH GRANT OPTION;</span><br></pre></td></tr></table></figure>
<h4 id="配置默认编码为-utf8">配置默认编码为 utf8</h4>
<p>MySQL 默认为 latin1, 一般修改为 UTF-8</p>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/my.cnf</span><br><span class="line">[mysqld]</span><br><span class="line"></span><br><span class="line">// 在myslqd下添加如下键值对</span><br><span class="line"></span><br><span class="line">character_set_server=utf8</span><br><span class="line">init_connect=<span class="string">&#x27;SET NAMES utf8&#x27;</span></span><br></pre></td></tr></table></figure>
<h4 id="重启-MySQL-服务，使配置生效">重启 MySQL 服务，使配置生效</h4>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl restart mysqld</span><br></pre></td></tr></table></figure>
<h4 id="开启端口">开启端口</h4>
<p>还要检查一下防火墙， 检查一下腾讯云的安全组</p>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo firewall-cmd --zone=public --add-port=3306/tcp --permanent</span><br><span class="line">$ sudo firewall-cmd --reload</span><br></pre></td></tr></table></figure>
<h2 id="安装-Nginx">安装 Nginx</h2>
<p>使用 yum 命令来安装 Nginx</p>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install -y nginx</span><br></pre></td></tr></table></figure>
<p>安装完成的 Nginx 并不会立刻启动，需要我们手动执行命令来开启它:</p>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl start nginx.service</span><br></pre></td></tr></table></figure>
<p>还可以输入以下命令，让 Nginx 可以随系统自动启动：</p>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl <span class="built_in">enable</span> nginx</span><br></pre></td></tr></table></figure>
<h4 id="相关补充">相关补充</h4>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开启 Nginx</span></span><br><span class="line">service nginx start</span><br><span class="line"><span class="comment"># 停止 Nginx</span></span><br><span class="line">service nginx stop</span><br><span class="line"><span class="comment"># 重启 Nginx</span></span><br><span class="line">service nginx restart</span><br><span class="line"><span class="comment"># 查看 Nginx 状态</span></span><br><span class="line">service nginx status</span><br></pre></td></tr></table></figure>
<p>Nginx 的默认站点根目录为</p>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/share/nginx/html/</span><br></pre></td></tr></table></figure>
<p>默认站点配置在</p>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/nginx/conf.d/default.conf</span><br></pre></td></tr></table></figure>
<p>Nginx 主配置如下</p>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/nginx/nginx.conf</span><br></pre></td></tr></table></figure>
<h4 id="防火墙指令">防火墙指令</h4>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">systemctl start firewalld.service  <span class="comment">#启动firewall</span></span><br><span class="line">systemctl stop firewalld.service  <span class="comment">#停止firewall</span></span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld.service  <span class="comment">#禁止firewall开机启动,enable</span></span><br><span class="line">firewall-cmd --state  <span class="comment">#防火墙状态</span></span><br><span class="line"></span><br><span class="line">firewall-cmd --permanent --zone=public --add-port=80/tcp  <span class="comment">#http协议基于TCP传输协议，放行80端口</span></span><br><span class="line">firewall-cmd --list-all  <span class="comment">#查看防火墙规则</span></span><br><span class="line">firewall-cmd --query-service nginx <span class="comment">#查看服务启动的状态</span></span><br><span class="line"></span><br><span class="line">firewall-cmd --add-service=ftp --permanent  <span class="comment">#永久开放ftp服务</span></span><br><span class="line">firewall-cmd --remove-service=ftp --permanent  <span class="comment">#永久关闭ftp服务</span></span><br><span class="line">firewall-cmd --get-service  <span class="comment">#查看服务名称</span></span><br></pre></td></tr></table></figure>
<h2 id="ssh免登陆">ssh免登陆</h2>
<h4 id="生成密钥对">生成密钥对</h4>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>
<h4 id="将公钥拷贝到目标主机">将公钥拷贝到目标主机</h4>
<p>用ssh登录到目标主机，然后cd ~/.ssh目录，如果目录不存在，那么要自己创建mkdir -p ~/.ssh。你今后要用哪个帐户登录主机，就在哪个帐户的home目录下操作，如果要免登陆root，就要去/root下操作。使用~比较好，不用多想了。</p>
<p>有了.ssh目录后，进去，然后把id_rsa.pub传过去，可以用scp命令，这里要做的一个主要操作，就是将id_rsa.pub，的文件内容，写到一个叫authorized_keys的文件中去，如果目标主机的相应用户名下已经有了.ssh目录和authorized_keys文件，那你操作要小心一点，可能别人也做过免登陆的设置，这个时候你要小心不要把别人的设置给覆盖了。如果没有的话，就创建文件touch ~/.ssh/authorized_keys，然后执行cat id_rsa.pub &gt;&gt; authorized_keys，将你的公钥写入到authorized_keys中，公钥文件.pub里面只有一行信息，上面的命令相当于把那一行信息追加到authorized_keys文件最后一行。</p>
<p>如果.ssh目录是你主机刚刚创建的，那么可能还需要改变一下这个目录的权限，将权限放低，chmod -R 0600 ~/.ssh，到此，所有设置就算做完了，你可以退出登录，在自己的主机上试一下了，现在再敲入ssh命令后，不用密码就可以登录主机了。</p>
<h2 id="pm2部署node应用到服务器">pm2部署node应用到服务器</h2>
<h4 id="三个概念">三个概念</h4>
<ul>
<li>Git服务器：用来保存你代码的仓库，比如：Github。也可以是你自己搭建的Git服务器。为了统一，后面全部都叫Github。</li>
<li>目标服务器：就是你要将你的项目部署到的那个服务器，以下统一都叫服务器（Server）。</li>
<li>本地环境：就是你开发用的PC，就叫它Local吧。</li>
</ul>
<h4 id="服务器端（Server）">服务器端（Server）</h4>
<p>服务端和本地需要安装node.js、pm2、git</p>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更新 yum</span></span><br><span class="line">$ yum update -y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 安装node.js</span></span><br><span class="line">$ yum install nodejs -y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 安装git</span></span><br><span class="line">$ yum install git -y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 安装pm2</span></span><br><span class="line">$ npm install -g pm2</span><br></pre></td></tr></table></figure>
<h4 id="在Github上添加Deploy-Keys">在Github上添加Deploy Keys</h4>
<p>在服务器（Server）上执行：</p>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成ssh key，一路回车即可</span></span><br><span class="line">$ ssh-keygen -t rsa</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看公钥内容</span></span><br><span class="line">$ <span class="built_in">cat</span> ~/.ssh/id_rsa.pub</span><br><span class="line"><span class="comment"># 然后复制整个内容，并添加到Github上对应的项目仓库Settings下的Deploy keys中。</span></span><br></pre></td></tr></table></figure>
<h2 id="pm2部署">pm2部署</h2>
<p>在项目的根目录下，执行：</p>
<p class="code-caption" data-lang="bash" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pm2 ecosystem</span><br></pre></td></tr></table></figure>
<p>接下来就看官方文档更好</p>
<p><a target="_blank" rel="noopener" href="http://pm2.keymetrics.io/docs/usage/deployment/">http://pm2.keymetrics.io/docs/usage/deployment/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2018/02/13/Nginx-Mysql-PM2-NODE-GIT-HTTPS%E5%BC%80%E5%8F%91%E6%97%A5%E8%AE%B0/" data-id="cl3jmf7ml002d9jv35hua8vrx" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-tensorflow笔记（9）" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/21/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%889%EF%BC%89/" class="article-date">
  <time datetime="2018-01-21T07:03:02.000Z" itemprop="datePublished">2018-01-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/21/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%889%EF%BC%89/">tensorflow笔记（9）</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2018/01/21/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%889%EF%BC%89/" data-id="cl3jmf7nk00679jv36g4m7q3h" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-tensorflow笔记（8）" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/19/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%888%EF%BC%89/" class="article-date">
  <time datetime="2018-01-19T04:07:05.000Z" itemprop="datePublished">2018-01-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/19/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%888%EF%BC%89/">tensorflow笔记（8）</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>####非监督学习  Autoencoder</p>
<p>今天我们会来聊聊用神经网络如何进行非监督形式的学习. 也就是 autoencoder, 自编码.</p>
<p>自编码 autoencoder 是一种什么码呢. 他是不是 条形码? 二维码? 打码? 其中的一种呢? NONONONO. 和他们统统没有关系. 自编码是一种神经网络的形式.如果你一定要把他们扯上关系, 我想也只能这样解释啦.</p>
<p>原来有时神经网络要接受大量的输入信息, 比如输入信息是高清图片时, 输入信息量可能达到上千万, 让神经网络直接从上千万个信息源中学习是一件很吃力的工作. 所以, 何不压缩一下, 提取出原图片中的最具代表性的信息, 缩减输入信息量, 再把缩减过后的信息放进神经网络学习. 这样学习起来就简单轻松了. 所以, 自编码就能在这时发挥作用. 通过将原数据白色的X 压缩, 解压 成黑色的X, 然后通过对比黑白 X ,求出预测误差, 进行反向传递, 逐步提升自编码的准确性. 训练好的自编码中间这一部分就是能总结原数据的精髓. 可以看出, 从头到尾, 我们只用到了输入数据 X, 并没有用到 X 对应的数据标签, 所以也可以说自编码是一种非监督学习. 到了真正使用自编码的时候. 通常只会用到自编码前半部分.</p>
<h4 id="编码器-Encoder">编码器 Encoder</h4>
<p>编码器能得到原数据的精髓, 然后我们只需要再创建一个小的神经网络学习这个精髓的数据,不仅减少了神经网络的负担, 而且同样能达到很好的效果.</p>
<p>他能从原数据中总结出每种类型数据的特征, 如果把这些特征类型都放在一张二维的图片上, 每种类型都已经被很好的用原数据的精髓区分开来. 如果你了解 PCA 主成分分析, 再提取主要特征时, 自编码和它一样,甚至超越了 PCA. 换句话说, 自编码 可以像 PCA 一样 给特征属性降维.</p>
<h4 id="解码器-Decoder">解码器 Decoder</h4>
<p>至于解码器 Decoder, 我们也能那它来做点事情. 我们知道, 解码器在训练的时候是要将精髓信息解压成原始信息, 那么这就提供了一个解压器的作用, 甚至我们可以认为是一个生成器 (类似于GAN). 那做这件事的一种特殊自编码叫做 variational autoencoders, 你能在这里找到他的具体说明.</p>
<h4 id="Autoencoder">Autoencoder</h4>
<p>Autoencoder 简单来说就是将有很多Feature的数据进行压缩，之后再进行解压的过程。 本质上来说，它也是一个对数据的非监督学习，如果大家知道 PCA (Principal component analysis)， 与 Autoencoder 相类似，它的主要功能即对数据进行非监督学习，并将压缩之后得到的“特征值”，这一中间结果正类似于PCA的结果。 之后再将压缩过的“特征值”进行解压，得到的最终结果与原始数据进行比较，对此进行非监督学习。如果大家还不是非常了解，请观看机器学习简介系列里的 Autoencoder 那一集； 如果对它已经有了一定的了解，那么便可以进行代码阶段的学习了。大概过程如下图所示：</p>
<p class="code-caption" data-lang="python" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">&quot;tmp/data/&quot;</span>, one_hot=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">5</span>  <span class="comment">#五组训练</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line">examples_to_show = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们的MNIST数据，每张图片大小是28x28 pix，即 784 Feature</span></span><br><span class="line"><span class="comment"># Network Parameters</span></span><br><span class="line">n_input = <span class="number">784</span>  <span class="comment"># MNIST data input (img shape: 28*28)</span></span><br><span class="line"><span class="comment"># tf Graph input (only pictures)</span></span><br><span class="line">X = tf.placeholder(<span class="string">&quot;float&quot;</span>, [<span class="literal">None</span>, n_input])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在压缩环节：我们要把这个Features不断压缩，</span></span><br><span class="line"><span class="comment"># 经过第一个隐藏层压缩至256个 Features，再经过第二个隐藏层压缩至128个。</span></span><br><span class="line"><span class="comment"># 在解压环节：我们将128个Features还原至256个，再经过一步还原至784个。</span></span><br><span class="line"><span class="comment"># 在对比环节：比较原始数据与还原后的拥有 784 Features 的数据进行 cost 的对比，</span></span><br><span class="line"><span class="comment"># 根据 cost 来提升我的 Autoencoder 的准确率，下图是两个隐藏层的 weights 和 biases 的定义：</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># hidden layer settings</span></span><br><span class="line">n_hidden_1 = <span class="number">256</span>  <span class="comment"># 1sr layer num features</span></span><br><span class="line">n_hidden_2 = <span class="number">128</span>  <span class="comment"># 2nd layer num features</span></span><br><span class="line">weights = &#123;</span><br><span class="line">    <span class="string">&#x27;encoder_h1&#x27;</span>: tf.Variable(tf.random_normal([n_input, n_hidden_1])),</span><br><span class="line">    <span class="string">&#x27;encoder_h2&#x27;</span>: tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),</span><br><span class="line">    <span class="string">&#x27;decoder_h1&#x27;</span>: tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),</span><br><span class="line">    <span class="string">&#x27;decoder_h2&#x27;</span>: tf.Variable(tf.random_normal([n_hidden_1, n_input])),</span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="string">&#x27;encoder_b1&#x27;</span>: tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">    <span class="string">&#x27;encoder_b2&#x27;</span>: tf.Variable(tf.random_normal([n_hidden_2])),</span><br><span class="line">    <span class="string">&#x27;decoder_b1&#x27;</span>: tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">    <span class="string">&#x27;decoder_b2&#x27;</span>: tf.Variable(tf.random_normal([n_input])),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面来定义Encoder和Decoder，使用的Activiation function是</span></span><br><span class="line"><span class="comment"># sigmoid, 压缩之后的值应该在[0,1]这个范围内。</span></span><br><span class="line"><span class="comment"># 在decoder过程中，通常使用对应与encoder的Activation function:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Building the encoder</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">encoder</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="comment"># Encoder Hidden layer with sigmoid activation #1</span></span><br><span class="line">    layer_1 = tf.nn.sigmoid(</span><br><span class="line">        tf.add(tf.matmul(x, weights[<span class="string">&#x27;encoder_h1&#x27;</span>]), biases[<span class="string">&#x27;encoder_b1&#x27;</span>]))</span><br><span class="line">    <span class="comment"># Decoder Hidden layer with sigmoid activation #2</span></span><br><span class="line">    layer_2 = tf.nn.sigmoid(</span><br><span class="line">        tf.add(</span><br><span class="line">            tf.matmul(layer_1, weights[<span class="string">&#x27;encoder_h2&#x27;</span>]), biases[<span class="string">&#x27;encoder_b2&#x27;</span>]))</span><br><span class="line">    <span class="keyword">return</span> layer_2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Building the decoder</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">decoder</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="comment"># Encoder Hidden layer with sigmoid activation #1</span></span><br><span class="line">    layer_1 = tf.nn.sigmoid(</span><br><span class="line">        tf.add(tf.matmul(x, weights[<span class="string">&#x27;decoder_h1&#x27;</span>]), biases[<span class="string">&#x27;decoder_b1&#x27;</span>]))</span><br><span class="line">    <span class="comment"># Decoder Hidden layer with sigmoid activation #2</span></span><br><span class="line">    layer_2 = tf.nn.sigmoid(</span><br><span class="line">        tf.add(</span><br><span class="line">            tf.matmul(layer_1, weights[<span class="string">&#x27;decoder_h2&#x27;</span>]), biases[<span class="string">&#x27;decoder_b2&#x27;</span>]))</span><br><span class="line">    <span class="keyword">return</span> layer_2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 来实现 Encoder 和 Decoder 输出的结果：</span></span><br><span class="line"><span class="comment"># Construct model</span></span><br><span class="line">encoder_op = encoder(X)  <span class="comment"># 128 Features</span></span><br><span class="line">decoder_op = decoder(encoder_op)  <span class="comment"># 784 Features</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Prediction</span></span><br><span class="line">y_pred = decoder_op  <span class="comment"># After</span></span><br><span class="line"><span class="comment"># Targets (Labels) are the input data.</span></span><br><span class="line">y_true = X  <span class="comment"># Before</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#再通过我们非监督学习进行对照，</span></span><br><span class="line"><span class="comment"># 即对 “原始的有 784 Features 的数据集” 和 “通过 ‘Prediction’</span></span><br><span class="line"><span class="comment"># 得出的有 784 Features 的数据集” 进行最小二乘法的计算，</span></span><br><span class="line"><span class="comment"># 并且使 cost 最小化:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define loss and optimizer, minimize the squared error</span></span><br><span class="line">cost = tf.reduce_mean(tf.<span class="built_in">pow</span>(y_true - y_pred, <span class="number">2</span>))</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后，通过Matplotlib的pyplot模块将结果显示出来，</span></span><br><span class="line"><span class="comment"># 注意在输出时MNIST数据集经过压缩之后x的最大值是1，而非255</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Launch the graph</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    total_batch = <span class="built_in">int</span>(mnist.train.num_examples / batch_size)</span><br><span class="line">    <span class="comment"># Training cycle</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(training_epochs):</span><br><span class="line">        <span class="comment"># Loop over all batches</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            <span class="comment"># Run optimization op (backprop) and cost op (to get loss value)</span></span><br><span class="line">            _, c = sess.run([optimizer, cost], feed_dict=&#123;X: batch_xs&#125;)</span><br><span class="line">            <span class="comment"># Display logs per epoch step</span></span><br><span class="line">            <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Epoch:&#x27;</span>, <span class="string">&#x27;%04d&#x27;</span> % (epoch + <span class="number">1</span>), <span class="string">&#x27;cost=&#x27;</span>,</span><br><span class="line">                      <span class="string">&#x27;&#123;:.9f&#125;&#x27;</span>.<span class="built_in">format</span>(c))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Optimization Finished!&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># # Applyinh encode and decode over test set</span></span><br><span class="line">    encode_decode = sess.run(</span><br><span class="line">        y_pred, feed_dict=&#123;</span><br><span class="line">            X: mnist.test.images[:examples_to_show]</span><br><span class="line">        &#125;)</span><br><span class="line">    <span class="comment"># Compare original imamges with their reconstructions</span></span><br><span class="line">    f, a = plt.subplots(<span class="number">2</span>, <span class="number">10</span>, figsize=(<span class="number">10</span>, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(examples_to_show):</span><br><span class="line">        a[<span class="number">0</span>][i].imshow(np.reshape(mnist.test.images[i], (<span class="number">28</span>, <span class="number">28</span>)))</span><br><span class="line">        a[<span class="number">1</span>][i].imshow(np.reshape(encode_decode[i], (<span class="number">28</span>, <span class="number">28</span>)))</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2018/01/19/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%888%EF%BC%89/" data-id="cl3jmf7nj00649jv3e5l2f1a1" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-tensorflow笔记（7）" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/13/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%887%EF%BC%89/" class="article-date">
  <time datetime="2018-01-13T13:47:35.000Z" itemprop="datePublished">2018-01-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/13/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%887%EF%BC%89/">tensorflow笔记（7）</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>###RNN LSTM（回归例子）</p>
<p>这次我们会使用 RNN 来进行回归的训练 (Regression). 会继续使用到自己创建的 sin 曲线预测一条 cos 曲线.</p>
<p class="code-caption" data-lang="python" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这次我们会使用 RNN 来进行回归的训练 (Regression).</span></span><br><span class="line"><span class="comment"># 会继续使用到自己创建的 sin 曲线预测一条 cos 曲线.</span></span><br><span class="line"><span class="comment"># 接下来我们先确定 RNN 的各种参数(super-parameters):</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">BATCH_START = <span class="number">0</span>  <span class="comment"># 建立 batch data 时候的 index</span></span><br><span class="line">TIME_STEPS = <span class="number">20</span>  <span class="comment"># backpropagation through time 的 time_steps</span></span><br><span class="line">BATCH_SIZE = <span class="number">50</span></span><br><span class="line">INPUT_SIZE = <span class="number">1</span>  <span class="comment"># sin数据输入 size</span></span><br><span class="line">OUTPUT_SIZE = <span class="number">1</span>  <span class="comment"># cos数据输出 size</span></span><br><span class="line">CELL_SIZE = <span class="number">10</span>  <span class="comment"># RNN的 hidden unit size</span></span><br><span class="line">LR = <span class="number">0.006</span>  <span class="comment"># learning rate</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 数据生成 ##</span></span><br><span class="line"><span class="comment"># 定义一个生成数据的 get_batch function：</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_batch</span>():</span><br><span class="line">    <span class="keyword">global</span> BATCH_START, TIME_STEPS</span><br><span class="line">    <span class="comment"># xs shape (50 batch, 20 steps)</span></span><br><span class="line">    xs = np.arange(BATCH_START, BATCH_START + TIME_STEPS * BATCH_SIZE).reshape(</span><br><span class="line">        (BATCH_SIZE, TIME_STEPS)) / (<span class="number">10</span> * np.pi)</span><br><span class="line">    seq = np.sin(xs)</span><br><span class="line">    res = np.cos(xs)</span><br><span class="line">    BATCH_START += TIME_STEPS</span><br><span class="line">    <span class="comment"># returned seq, res and xs: shape (batch, step, input)</span></span><br><span class="line">    <span class="keyword">return</span> [seq[:, :, np.newaxis], res[:, :, np.newaxis], xs]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 定义LSTMRNN 的主体结构 ##</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LSTMRNN</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_steps, input_size, output_size, cell_size,</span></span><br><span class="line"><span class="params">                 batch_size</span>):</span><br><span class="line"></span><br><span class="line">        self.n_steps = n_steps</span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.output_size = output_size</span><br><span class="line">        self.cell_size = cell_size</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;inputs&#x27;</span>):</span><br><span class="line">            self.xs = tf.placeholder(</span><br><span class="line">                tf.float32, [<span class="literal">None</span>, n_steps, input_size], name=<span class="string">&#x27;xs&#x27;</span>)</span><br><span class="line">            self.ys = tf.placeholder(</span><br><span class="line">                tf.float32, [<span class="literal">None</span>, n_steps, output_size], name=<span class="string">&#x27;ys&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;in_hidden&#x27;</span>):</span><br><span class="line">            self.add_input_layer()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;LSTM_cell&#x27;</span>):</span><br><span class="line">            self.add_cell()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;out_hidden&#x27;</span>):</span><br><span class="line">            self.add_output_layer()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;cost&#x27;</span>):</span><br><span class="line">            self.compute_cost()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;train&#x27;</span>):</span><br><span class="line">            self.train_op = tf.train.AdamOptimizer(LR).minimize(self.cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置 add_input_layer 功能, 添加 input_layer:</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_input_layer</span>(<span class="params">self, </span>):</span><br><span class="line"></span><br><span class="line">        l_in_x = tf.reshape(</span><br><span class="line">            self.xs, [-<span class="number">1</span>, self.input_size],</span><br><span class="line">            name=<span class="string">&#x27;2_2D&#x27;</span>)  <span class="comment">#  (batch*n_step, in_size)</span></span><br><span class="line">        <span class="comment"># Ws (in_size, cell_size)</span></span><br><span class="line">        Ws_in = self._weight_variable([self.input_size, self.cell_size])</span><br><span class="line">        <span class="comment"># bs (cell_size, )</span></span><br><span class="line">        bs_in = self._bias_variable([self.cell_size])</span><br><span class="line">        <span class="comment"># l_in_y = (batch * n_steps, cell_size)</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;Wx_plus_b&#x27;</span>):</span><br><span class="line">            l_in_y = tf.matmul(l_in_x, Ws_in) + bs_in</span><br><span class="line">        <span class="comment">#reshape l_in_y ==&gt; (batch, n_steps, cell_size)</span></span><br><span class="line">        self.l_in_y = tf.reshape(</span><br><span class="line">            l_in_y, [-<span class="number">1</span>, self.n_steps, self.cell_size], name=<span class="string">&#x27;2_3D&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置 add_cell 功能, 添加 cell,</span></span><br><span class="line">    <span class="comment"># 注意这里的 self.cell_init_state,</span></span><br><span class="line">    <span class="comment"># 因为我们在 training 的时候, 这个地方要特别说明.</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_cell</span>(<span class="params">self</span>):</span><br><span class="line"></span><br><span class="line">        lstm_cell = tf.contrib.rnn.BasicLSTMCell(</span><br><span class="line">            self.cell_size, forget_bias=<span class="number">1.0</span>, state_is_tuple=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;initial_state&#x27;</span>):</span><br><span class="line">            self.cell_init_state = lstm_cell.zero_state(</span><br><span class="line">                self.batch_size, dtype=tf.float32)</span><br><span class="line">            self.cell_outputs, self.cell_final_state = tf.nn.dynamic_rnn(</span><br><span class="line">                lstm_cell,</span><br><span class="line">                self.l_in_y,</span><br><span class="line">                initial_state=self.cell_init_state,</span><br><span class="line">                time_major=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置 add_output_layer 功能, 添加 output_layer:</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_output_layer</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># shape = (batch * steps, cell_size)</span></span><br><span class="line">        l_out_x = tf.reshape(</span><br><span class="line">            self.cell_outputs, [-<span class="number">1</span>, self.cell_size], name=<span class="string">&#x27;2_2D&#x27;</span>)</span><br><span class="line">        Ws_out = self._weight_variable([self.cell_size, self.output_size])</span><br><span class="line">        bs_out = self._bias_variable([</span><br><span class="line">            self.output_size,</span><br><span class="line">        ])</span><br><span class="line">        <span class="comment"># shape = (batch * steps, out_size)</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;Wx_plus_b&#x27;</span>):</span><br><span class="line">            self.pred = tf.matmul(l_out_x, Ws_out) + bs_out</span><br><span class="line"></span><br><span class="line">    <span class="comment">#添加RNN中剩下的部分：</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">self</span>):</span><br><span class="line">        losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example(</span><br><span class="line">            [tf.reshape(self.pred, [-<span class="number">1</span>], name=<span class="string">&#x27;reshape_pred&#x27;</span>)],</span><br><span class="line">            [tf.reshape(self.ys, [-<span class="number">1</span>], name=<span class="string">&#x27;reshape_target&#x27;</span>)],</span><br><span class="line">            [tf.ones([self.batch_size * self.n_steps], dtype=tf.float32)],</span><br><span class="line">            average_across_timesteps=<span class="literal">True</span>,</span><br><span class="line">            softmax_loss_function=self.ms_error,</span><br><span class="line">            name=<span class="string">&#x27;losses&#x27;</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;average_cost&#x27;</span>):</span><br><span class="line">            self.cost = tf.div(</span><br><span class="line">                tf.reduce_sum(losses, name=<span class="string">&#x27;losses_sum&#x27;</span>),</span><br><span class="line">                self.batch_size,</span><br><span class="line">                name=<span class="string">&#x27;average_cost&#x27;</span>)</span><br><span class="line">            tf.summary.scalar(<span class="string">&#x27;cost&#x27;</span>, self.cost)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">ms_error</span>(<span class="params">labels, logits</span>):</span><br><span class="line">        <span class="keyword">return</span> tf.square(tf.subtract(labels, logits))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_weight_variable</span>(<span class="params">self, shape, name=<span class="string">&#x27;weights&#x27;</span></span>):</span><br><span class="line">        initializer = tf.random_normal_initializer(</span><br><span class="line">            mean=<span class="number">0.</span>,</span><br><span class="line">            stddev=<span class="number">1.</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> tf.get_variable(shape=shape, initializer=initializer, name=name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_bias_variable</span>(<span class="params">self, shape, name=<span class="string">&#x27;biases&#x27;</span></span>):</span><br><span class="line">        initializer = tf.constant_initializer(<span class="number">0.1</span>)</span><br><span class="line">        <span class="keyword">return</span> tf.get_variable(name=name, shape=shape, initializer=initializer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 训练 LSTMRNN ##</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment">#搭建 LSTMRNN 模型</span></span><br><span class="line">    model = LSTMRNN(TIME_STEPS, INPUT_SIZE, OUTPUT_SIZE, CELL_SIZE, BATCH_SIZE)</span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    <span class="comment"># sess.run(tf.initialize_all_variables()) # tf 马上就要废弃这种写法</span></span><br><span class="line">    <span class="comment"># 替换成下面的写法：</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># matplotlib 可视化</span></span><br><span class="line">    plt.ion()  <span class="comment"># 设置连续 plot</span></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练 200 次</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):</span><br><span class="line">        seq, res, xs = get_batch()  <span class="comment"># 提取 batch data</span></span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 初始化data</span></span><br><span class="line">            feed_dict = &#123;model.xs: seq, model.ys: res&#125;</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            feed_dict = &#123;</span><br><span class="line">                model.xs: seq,</span><br><span class="line">                model.ys: res,</span><br><span class="line">                model.cell_init_state: state  <span class="comment"># 保持 state 的连续性</span></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练</span></span><br><span class="line">        _, cost, state, pred = sess.run(</span><br><span class="line">            [model.train_op, model.cost, model.cell_final_state, model.pred],</span><br><span class="line">            feed_dict=feed_dict)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># plotting</span></span><br><span class="line">        plt.plot(xs[<span class="number">0</span>, :], res[<span class="number">0</span>].flatten(), <span class="string">&#x27;r&#x27;</span>, xs[<span class="number">0</span>, :],</span><br><span class="line">                 pred.flatten()[:TIME_STEPS], <span class="string">&#x27;b--&#x27;</span>)</span><br><span class="line">        plt.ylim((-<span class="number">1.2</span>, <span class="number">1.2</span>))</span><br><span class="line">        plt.draw()</span><br><span class="line">        plt.pause(<span class="number">0.3</span>)  <span class="comment"># 每 0.3 s 刷新一次</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打印 cost 结构</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;cost:&#x27;</span>, <span class="built_in">round</span>(cost, <span class="number">4</span>))</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2018/01/13/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%887%EF%BC%89/" data-id="cl3jmf7nj00619jv35f9ahij2" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-tensorflow笔记（6）" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/12/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%886%EF%BC%89/" class="article-date">
  <time datetime="2018-01-12T05:54:58.000Z" itemprop="datePublished">2018-01-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/12/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%886%EF%BC%89/">tensorflow笔记（6）</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="序列数据">序列数据</h3>
<p>我们想象现在有一组序列数据 data 0,1,2,3. 在当预测 result0 的时候,我们基于的是 data0, 同样在预测其他数据的时候, 我们也都只单单基于单个的数据. 每次使用的神经网络都是同一个 NN. 不过这些数据是有关联 顺序的 , 就像在厨房做菜, 酱料 A要比酱料 B 早放, 不然就串味了. 所以普通的神经网络结构并不能让 NN 了解这些数据之间的关联.</p>
<h3 id="处理序列数据的神经网络">处理序列数据的神经网络</h3>
<p>那我们如何让数据间的关联也被 NN 加以分析呢? 想想我们人类是怎么分析各种事物的关联吧, 最基本的方式,就是记住之前发生的事情. 那我们让神经网络也具备这种记住之前发生的事的能力. 再分析 Data0 的时候, 我们把分析结果存入记忆. 然后当分析 data1的时候, NN会产生新的记忆, 但是新记忆和老记忆是没有联系的. 我们就简单的把老记忆调用过来, 一起分析. 如果继续分析更多的有序数据 , RNN就会把之前的记忆都累积起来, 一起分析.</p>
<p>我们再重复一遍刚才的流程, 不过这次是以加入一些数学方面的东西. 每次 RNN 运算完之后都会产生一个对于当前状态的描述 , state. 我们用简写 S( t) 代替, 然后这个 RNN开始分析 x(t+1) , 他会根据 x(t+1)产生s(t+1), 不过此时 y(t+1) 是由 s(t) 和 s(t+1) 共同创造的. 所以我们通常看到的 RNN 也可以表达成这种样子.</p>
<h3 id="RNN的弊端">RNN的弊端</h3>
<p>之前我们说过, RNN 是在有顺序的数据上进行学习的. 为了记住这些数据, RNN 会像人一样产生对先前发生事件的记忆. 不过一般形式的 RNN 就像一个老爷爷, 有时候比较健忘. 为什么会这样呢?</p>
<p>想像现在有这样一个 RNN, 他的输入值是一句话: ‘我今天要做红烧排骨, 首先要准备排骨, 然后…., 最后美味的一道菜就出锅了’, shua ~ 说着说着就流口水了. 现在请 RNN 来分析, 我今天做的到底是什么菜呢. RNN可能会给出“辣子鸡”这个答案. 由于判断失误, RNN就要开始学习 这个长序列 X 和 ‘红烧排骨’ 的关系 , 而RNN需要的关键信息 ”红烧排骨”却出现在句子开头,</p>
<p>再来看看 RNN是怎样学习的吧. 红烧排骨这个信息原的记忆要进过长途跋涉才能抵达最后一个时间点. 然后我们得到误差, 而且在 反向传递 得到的误差的时候, 他在每一步都会 乘以一个自己的参数 W. 如果这个 W 是一个小于1 的数, 比如0.9. 这个0.9 不断乘以误差, 误差传到初始时间点也会是一个接近于零的数, 所以对于初始时刻, 误差相当于就消失了. 我们把这个问题叫做梯度消失或者梯度弥散 Gradient vanishing. 反之如果 W 是一个大于1 的数, 比如1.1 不断累乘, 则到最后变成了无穷大的数, RNN被这无穷大的数撑死了, 这种情况我们叫做剃度爆炸, Gradient exploding. 这就是普通 RNN 没有办法回忆起久远记忆的原因.</p>
<h3 id="LSTM">LSTM</h3>
<p>LSTM 就是为了解决这个问题而诞生的. LSTM 和普通 RNN 相比, 多出了三个控制器. (输入控制, 输出控制, 忘记控制). 现在, LSTM RNN 内部的情况是这样.</p>
<p>他多了一个 控制全局的记忆, 我们用粗线代替. 为了方便理解, 我们把粗线想象成电影或游戏当中的 主线剧情. 而原本的 RNN 体系就是 分线剧情. 三个控制器都是在原始的 RNN 体系上, 我们先看 输入方面 , 如果此时的分线剧情对于剧终结果十分重要, 输入控制就会将这个分线剧情按重要程度 写入主线剧情 进行分析. 再看 忘记方面, 如果此时的分线剧情更改了我们对之前剧情的想法, 那么忘记控制就会将之前的某些主线剧情忘记, 按比例替换成现在的新剧情. 所以 主线剧情的更新就取决于输入 和忘记 控制. 最后的输出方面, 输出控制会基于目前的主线剧情和分线剧情判断要输出的到底是什么.基于这些控制机制, LSTM 就像延缓记忆衰退的良药, 可以带来更好的结果.</p>
<h3 id="实现例子">实现例子</h3>
<p class="code-caption" data-lang="python" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这次我们会使用 RNN 来进行分类的训练 (Classification).</span></span><br><span class="line"><span class="comment"># 会继续使用到手写数字 MNIST 数据集.</span></span><br><span class="line"><span class="comment"># 让 RNN 从每张图片的第一行像素读到最后一行, 然后再进行分类判断.</span></span><br><span class="line"><span class="comment"># 接下来我们导入 MNIST 数据并确定 RNN 的各种参数(hyper-parameters):</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">tf.set_random_seed(<span class="number">1</span>)  <span class="comment">#set random seed</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入数据</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">&#x27;MNIST_data&#x27;</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># hyperparameters</span></span><br><span class="line">lr = <span class="number">0.001</span>  <span class="comment"># learning rate</span></span><br><span class="line">training_iters = <span class="number">100000</span>  <span class="comment"># train step 上限</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">n_inputs = <span class="number">28</span>  <span class="comment"># MNIST data input(img shape: 28*28)</span></span><br><span class="line">n_steps = <span class="number">28</span>  <span class="comment"># time step</span></span><br><span class="line">n_hidden_units = <span class="number">128</span>  <span class="comment"># neurons in hidden layer</span></span><br><span class="line">n_classes = <span class="number">10</span>  <span class="comment"># MNIST classes (0-9 digits)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 接着定义x，y的placeholder和weights，biases的初始状况</span></span><br><span class="line"><span class="comment"># x y placeholder</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, n_steps, n_inputs])</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="literal">None</span>, n_classes])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对 weights biases 初始值对定义</span></span><br><span class="line"></span><br><span class="line">weights = &#123;</span><br><span class="line">    <span class="comment"># shape (28, 128)</span></span><br><span class="line">    <span class="string">&#x27;in&#x27;</span>: tf.Variable(tf.random_normal([n_inputs, n_hidden_units])),</span><br><span class="line">    <span class="comment"># shape(128 , 10)</span></span><br><span class="line">    <span class="string">&#x27;out&#x27;</span>: tf.Variable(tf.random_normal([n_hidden_units, n_classes]))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="comment"># shape (128, )</span></span><br><span class="line">    <span class="string">&#x27;in&#x27;</span>: tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[</span><br><span class="line">        n_hidden_units,</span><br><span class="line">    ])),</span><br><span class="line">    <span class="comment"># shape (10, )</span></span><br><span class="line">    <span class="string">&#x27;out&#x27;</span>: tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[</span><br><span class="line">        n_classes,</span><br><span class="line">    ]))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 RNN 的主体结构</span></span><br><span class="line"><span class="comment"># 接着开始定义RNN主体结构， 这个RNN总共有3个组成部分（input_layer, cell, output_layer)</span></span><br><span class="line"><span class="comment"># 首先我们线定义input——layer</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">RNN</span>(<span class="params">X, weights, biases</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 原始对 X 是 3 维数据，我们需要把它变成 2 维数据才能使用weights对矩阵乘法</span></span><br><span class="line">    <span class="comment"># X ==&gt; (128 batch * 28 steps, 28 inputs)</span></span><br><span class="line">    X = tf.reshape(X, [-<span class="number">1</span>, n_inputs])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># X_in = W * X + b</span></span><br><span class="line">    <span class="comment"># X_in = (128 batch * 28 steps, 128 hidden)</span></span><br><span class="line">    X_in = tf.matmul(X, weights[<span class="string">&#x27;in&#x27;</span>]) + biases[<span class="string">&#x27;in&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># X_in ==&gt; (128 batch, 28 steps, 128 hidden) 换回3维</span></span><br><span class="line">    X_in = tf.reshape(X_in, [-<span class="number">1</span>, n_steps, n_hidden_units])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接着是 cell 中的计算, 有两种途径:</span></span><br><span class="line">    <span class="comment"># 使用 tf.nn.rnn(cell, inputs) (不推荐原因). 但是如果使用这种方法, 可以参考原因;</span></span><br><span class="line">    <span class="comment"># 使用 tf.nn.dynamic_rnn(cell, inputs) (推荐). 这次的练习将使用这种方式.</span></span><br><span class="line">    <span class="comment"># 因 Tensorflow 版本升级原因, state_is_tuple=True 将在之后的版本中变为默认.</span></span><br><span class="line">    <span class="comment"># 对于 lstm 来说, state可被分为(c_state, h_state).</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用 basic LSTM Cell。</span></span><br><span class="line">    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_units)</span><br><span class="line">    init_state = lstm_cell.zero_state(</span><br><span class="line">        batch_size, dtype=tf.float32)  <span class="comment"># 初始化全零 state</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果使用tf.nn.dynamic_rnn(cell, inputs), 我们要确定 inputs 的格式.</span></span><br><span class="line">    <span class="comment"># tf.nn.dynamic_rnn 中的 time_major 参数会针对不同 inputs 格式有不同的值.</span></span><br><span class="line">    <span class="comment"># 如果 inputs 为 (batches, steps, inputs) ==&gt; time_major=False;</span></span><br><span class="line">    <span class="comment"># 如果 inputs 为 (steps, batches, inputs) ==&gt; time_major=True;</span></span><br><span class="line"></span><br><span class="line">    outputs, final_state = tf.nn.dynamic_rnn(</span><br><span class="line">        lstm_cell, X_in, initial_state=init_state, time_major=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 方式一: 直接调用final_state 中的 h_state (final_state[1]) 来进行运算:</span></span><br><span class="line">    <span class="comment"># results = tf.matmul(final_state[1], weights[&#x27;out&#x27;]) + biases[&#x27;out&#x27;]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 方式二: 调用最后一个outputs（在这个例子中，和上面的final_state[1]是一样的）</span></span><br><span class="line">    <span class="comment"># outputs 变成列表 [(batch, outputs) ..] * steps</span></span><br><span class="line">    outputs = tf.unstack(tf.transpose(outputs, [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">    results = tf.matmul(outputs[-<span class="number">1</span>],</span><br><span class="line">                        weights[<span class="string">&#x27;out&#x27;</span>]) + biases[<span class="string">&#x27;out&#x27;</span>]  <span class="comment">#选取最后一个output</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#最后输出result</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义好了RNN主体结构后，我们就可以来计算cost和train_op</span></span><br><span class="line">pred = RNN(x, weights, biases)</span><br><span class="line">cost = tf.reduce_mean(</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))</span><br><span class="line">train_op = tf.train.AdamOptimizer(lr).minimize(cost)</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练时，不断输出accuract，观看结果：</span></span><br><span class="line"></span><br><span class="line">correct_pred = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> step * batch_size &lt; training_iters:</span><br><span class="line"></span><br><span class="line">        batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">        batch_xs = batch_xs.reshape([batch_size, n_steps, n_inputs])</span><br><span class="line">        sess.run([train_op], feed_dict=&#123;x: batch_xs, y: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys&#125;))</span><br><span class="line">        step += <span class="number">1</span></span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2018/01/12/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%886%EF%BC%89/" data-id="cl3jmf7ni005y9jv3d2i590qc" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-tensorflow笔记（5）" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/11/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89/" class="article-date">
  <time datetime="2018-01-11T15:52:54.000Z" itemprop="datePublished">2018-01-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/11/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89/">tensorflow笔记（5）</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Saver-保存读取">Saver 保存读取</h4>
<p>我们搭建好了一个神经网络, 训练好了, 肯定也想保存起来, 用于再次加载. 那今天我们就来说说怎样用 Tensorflow 中的 saver 保存和加载吧.</p>
<p class="code-caption" data-lang="python" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 保存神经网络的方法</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">## Save to file</span></span><br><span class="line"><span class="comment"># remember to define the same dtype and shape when restore</span></span><br><span class="line"><span class="comment"># W = tf.Variable([[1, 2, 3], [3, 4, 5]], dtype=tf.float32, name=&#x27;weights&#x27;)</span></span><br><span class="line"><span class="comment"># b = tf.Variable([[1, 2, 3]], dtype=tf.float32, name=&#x27;biases&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># init = tf.global_variables_initializer()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 保存时，首先要先建立一个tf.train.Saver()用来保存，提取变量</span></span><br><span class="line"><span class="comment"># # 再创建一个名为my_net的文件夹，用这个saver来保存变量道这个目录</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># saver = tf.train.Saver()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># with tf.Session() as sess:</span></span><br><span class="line"><span class="comment">#     sess.run(init)</span></span><br><span class="line"><span class="comment">#     save_path = saver.save(sess, &quot;my_net/save_net.ckpt&quot;)</span></span><br><span class="line"><span class="comment">#     print(&quot;Save to path:&quot;, save_path)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 提取</span></span><br><span class="line"><span class="comment"># # 先建立W，b道容器</span></span><br><span class="line">W = tf.Variable(np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>, <span class="number">3</span>)), dtype=tf.float32, name=<span class="string">&quot;weights&quot;</span>)</span><br><span class="line">b = tf.Variable(np.arange(<span class="number">3</span>).reshape((<span class="number">1</span>, <span class="number">3</span>)), dtype=tf.float32, name=<span class="string">&quot;biases&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里不需要初始化步骤 init=tf.initialize_all_variables()</span></span><br><span class="line"></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 提取变量</span></span><br><span class="line">    saver.restore(sess, <span class="string">&quot;my_net/save_net.ckpt&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;weights:&quot;</span>, sess.run(W))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;biases:&quot;</span>, sess.run(b))</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2018/01/11/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89/" data-id="cl3jmf7nh005v9jv37d06ez0n" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-tensorflow笔记（4）" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/10/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%884%EF%BC%89/" class="article-date">
  <time datetime="2018-01-10T05:16:59.000Z" itemprop="datePublished">2018-01-10</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Tensorflow/">Tensorflow</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/10/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%884%EF%BC%89/">tensorflow笔记（4）</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="一个简单的基于MNIST训练的CNN模型">一个简单的基于MNIST训练的CNN模型</h3>
<p>卷积神经网络包含输入层、隐藏层和输出层，隐藏层又包含卷积层和pooling层，图像输入到卷积神经网络后通过卷积来不断的提取特征，每提取一个特征就会增加一个feature map，pooling层也就是下采样，通常采用的是最大值pooling和平均值pooling，因为参数太多喽，所以通过pooling来稀疏参数，使我们的网络不至于太复杂。</p>
<p>好啦，既然你对卷积神经网络已经有了大概的了解，下次课我们将通过代码来实现一个基于MNIST数据集的简单卷积神经网络。</p>
<p class="code-caption" data-lang="python" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_accuracy</span>(<span class="params">v_xs, v_ys</span>):</span><br><span class="line">    <span class="keyword">global</span> prediction</span><br><span class="line">    y_pre = sess.run(prediction, feed_dict=&#123;xs: v_xs, keep_prob: <span class="number">1</span>&#125;)</span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(y_pre, <span class="number">1</span>), tf.argmax(v_ys, <span class="number">1</span>))</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">    result = sess.run(accuracy, feed_dict=&#123;xs: v_xs, ys: v_ys, keep_prob: <span class="number">1</span>&#125;)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们定义Weight变量，输入shape，返回变量的参数。其中我们使用tf.truncted_normal产生随机变量来进行初始化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">weight_variable</span>(<span class="params">shape</span>):</span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 同样的定义biase变量</span></span><br><span class="line"><span class="comment"># 输入shape ，返回变量的一些参数。其中我们使用tf.constant常量函数来进行初始化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bias_variable</span>(<span class="params">shape</span>):</span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义卷积，tf.nn.conv2d函数是tensorflow里面的二维卷积函数</span></span><br><span class="line"><span class="comment"># x是图片的所有参数，W是次卷积层的权重，然后定义步长strides=[1,1,1,1]的值</span></span><br><span class="line"><span class="comment"># strides[0]和strides[3]的两个1是默认值</span></span><br><span class="line"><span class="comment"># 中间两个1代表padding时在x方向运动一步</span></span><br><span class="line"><span class="comment"># x方向运动一步，y方向运动一步，padding采用的方式是SAME</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv2d</span>(<span class="params">x, W</span>):</span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义池化pooling</span></span><br><span class="line"><span class="comment"># padding时，我们选的是一次一步，这样得到的图片尺寸没有变化</span></span><br><span class="line"><span class="comment"># 而我们希望压缩一下图片也就是参数能少一些从而减少系统复杂度</span></span><br><span class="line"><span class="comment"># 因此我们采用pooling来稀疏化参数，也就是卷积神经网络中所谓的下采样层。</span></span><br><span class="line"><span class="comment"># pooling 有两种，一种是最大值池化，一种是平均值池化，</span></span><br><span class="line"><span class="comment"># 本例采用的是最大值池化tf.max_pool()。</span></span><br><span class="line"><span class="comment"># 池化的核函数大小为2x2，因此ksize=[1,2,2,1]，步长为2，因此strides=[1,2,2,1]:</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">max_pool_2x2</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(</span><br><span class="line">        x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据来源</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">&#x27;MNIST_data&#x27;</span>, one_hot=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入的placeholder</span></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>]) / <span class="number">255</span>  <span class="comment"># 28x28</span></span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># -1代表先不考虑输入的图片例子多少这个维度</span></span><br><span class="line"><span class="comment"># 后面的1是channel的数量，因为我们输入的图片是黑白的，因此channel是1，例如如果是RGB图像，那么channel就是3。</span></span><br><span class="line">x_image = tf.reshape(xs, [-<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">## conv1 layer --- start ##</span></span><br><span class="line"><span class="comment"># 建立卷积层</span></span><br><span class="line"><span class="comment"># 定义第一层卷积，先定义本层的Weight，本层我们的卷积层核patch的大小是5x5</span></span><br><span class="line"><span class="comment"># 因为channel是1，所有输入是1， 输出是32个featuremap</span></span><br><span class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接着定义bias，它的大小是32个长度，因此我们传入它的shape为[32]</span></span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line"><span class="comment"># 第一个卷积层</span></span><br><span class="line"><span class="comment"># h_conv1=conv2d(x_image,W_conv1)+b_conv1</span></span><br><span class="line"><span class="comment"># 同时我们对h_conv1进行非线性处理，也就是激活函数来处理喽，</span></span><br><span class="line"><span class="comment"># 这里我们用的是tf.nn.relu（修正线性单元）来处理，</span></span><br><span class="line"><span class="comment"># 要注意的是，因为采用了SAME的padding方式，</span></span><br><span class="line"><span class="comment"># 输出图片的大小没有变化依然是28x28，只是厚度变厚了，</span></span><br><span class="line"><span class="comment"># 因此现在的输出大小就变成了28x28x32</span></span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line"></span><br><span class="line"><span class="comment">#最后我们再进行pooling的处理就ok啦，经过pooling的处理，输出大小就变为了14x14x32</span></span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line"></span><br><span class="line"><span class="comment">## conv1 layer --- end ##</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## conv2 layer --- start ##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 接着呢，同样的形式我们定义第二层卷积，</span></span><br><span class="line"><span class="comment"># 本层我们的输入就是上一层的输出，本层我们的卷积核patch的大小是5x5，</span></span><br><span class="line"><span class="comment"># 有32个featuremap所以输入就是32，输出呢我们定为64</span></span><br><span class="line">W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line"><span class="comment"># 接着我们就可以定义卷积神经网络的第二个卷积层，这时的输出的大小就是14x14x64</span></span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line"><span class="comment"># 最后也是一个pooling处理，输出大小为7x7x64</span></span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)</span><br><span class="line"><span class="comment"># 建立全连接层</span></span><br><span class="line"><span class="comment"># 进入全连接层时，我们通过tf.reshape()将h_pool2的输出值从一个三维的变为一维的数据</span></span><br><span class="line"><span class="comment"># 表示先不考虑输入图片例子纬度，将上一个输出结果展平</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## conv1 layer --- end ##</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## fc1 layer --- start ##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 此时weight_variable的shape输入就是第二个卷积层展平的输出大小7*7*64</span></span><br><span class="line"><span class="comment"># 后面输出的size我们继续扩大，定为1024</span></span><br><span class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line"><span class="comment">#[n_sample, 7,7,64] -&gt;&gt; [n_samples. 7*6*64]</span></span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [-<span class="number">1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</span><br><span class="line"><span class="comment"># 然后将展平后的h_pool2_flat与本层的W_fc1相乘（注意这个时候不是卷积了）</span></span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line"><span class="comment"># 定义dropout的placeholder，它是解决过拟合的有效手段</span></span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line"><span class="comment"># 如果我们考虑过拟合的问题，可以加一个dropout的处理</span></span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line"></span><br><span class="line"><span class="comment">## fc1 layer --- end ##</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## fc2 layer --- start ##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来我们就可以进行最后一层的构建了，好激动啊，输入是1024</span></span><br><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line"><span class="comment"># 然后呢，我们用softmax分类器（多分类，输出的是各个类的概率），对我们的输出进行分类</span></span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</span><br><span class="line"></span><br><span class="line"><span class="comment">## fc2 layer --- end ##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 选优化方法</span></span><br><span class="line">cross_entropy = tf.reduce_mean(</span><br><span class="line">    -tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[<span class="number">1</span>]))  <span class="comment"># loss</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们用tf.train.AdamOptimizer()作为我们的优化器进行优化，使我们的cross_entropy最小</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接着呢就是和之前视频讲的一样喽， 定义Session</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化变量</span></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    sess.run(</span><br><span class="line">        train_step, feed_dict=&#123;</span><br><span class="line">            xs: batch_xs,</span><br><span class="line">            ys: batch_ys,</span><br><span class="line">            keep_prob: <span class="number">0.5</span></span><br><span class="line">        &#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            compute_accuracy(mnist.test.images[:<span class="number">1000</span>],</span><br><span class="line">                             mnist.test.labels[:<span class="number">1000</span>]))</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2018/01/10/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%884%EF%BC%89/" data-id="cl3jmf7nh005s9jv37u349x1l" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-TensorFlow-Classification分类学习" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/08/TensorFlow-Classification%E5%88%86%E7%B1%BB%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time datetime="2018-01-08T12:43:40.000Z" itemprop="datePublished">2018-01-08</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Tensorflow/">Tensorflow</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/01/08/TensorFlow-Classification%E5%88%86%E7%B1%BB%E5%AD%A6%E4%B9%A0/">TensorFlow Classification分类学习</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="使用MNIST实现的例子">使用MNIST实现的例子</h4>
<p>具体的实现在注释里详细说明了</p>
<p class="code-caption" data-lang="python" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#添加神经层的函数，有四个参数</span></span><br><span class="line"><span class="comment">#输入值，输入的大小，输出的大小和激励函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_layer</span>(<span class="params">inputs, in_size, out_size, activation_function=<span class="literal">None</span></span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment">#在生成初始参数的时候，使用随机变量（normal distribution）会比全部为0要好很多</span></span><br><span class="line">    <span class="comment">#这里的weights为一个in_size行，out_size列的随机变量矩阵</span></span><br><span class="line">    Weights = tf.Variable(tf.random_normal([in_size, out_size]))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#在机器学习中，biases的推荐值不为0，所以我们这里是在0向量的基础上又加了0.1</span></span><br><span class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#下面，我们定义Wx_plus_b，即神经网络未激活的值。其中，tf.matmul()是矩阵乘法</span></span><br><span class="line">    <span class="comment">#矩阵乘法是列与行的乘法</span></span><br><span class="line">    Wx_plus_b = tf.matmul(inputs, Weights) + biases</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        outputs = Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs = activation_function(Wx_plus_b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_accuracy</span>(<span class="params">v_xs, v_ys</span>):</span><br><span class="line">    <span class="keyword">global</span> prediction</span><br><span class="line">    y_pre = sess.run(prediction, feed_dict=&#123;xs: v_xs&#125;)</span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(y_pre, <span class="number">1</span>), tf.argmax(v_ys, <span class="number">1</span>))</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">    result = sess.run(accuracy, feed_dict=&#123;xs: v_xs, ys: v_ys&#125;)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#导入数据（MNIST库)</span></span><br><span class="line"><span class="comment">#图片为28 * 28 = 784</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">&#x27;MNIST_data&#x27;</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#每张图片都表示一个数字，所以我们的输出是数字0到9，共10类</span></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>])  <span class="comment"># 28x28</span></span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#调用add_layer函数搭建一个最简单的训练网络结构，只有输入层和输出层。</span></span><br><span class="line"><span class="comment">#其中输入数据是784个特征，输出数据是10个特征，激励采用softmax函数，网络结构图是这样子的</span></span><br><span class="line">prediction = add_layer(xs, <span class="number">784</span>, <span class="number">10</span>, activation_function=tf.nn.softmax)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Cross entropy loss</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#loss函数（即最优化目标函数）选用交叉熵函数。</span></span><br><span class="line"><span class="comment">#交叉熵用来衡量预测值和真实值的相似程度，如果完全相同，它们的交叉熵等于零</span></span><br><span class="line"></span><br><span class="line">cross_entropy = tf.reduce_mean(</span><br><span class="line">    -tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy)</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 替换成下面的写法:</span></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line"><span class="comment">#现在开始train，每次只取100张图片，免得数据太多训练太慢。</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;xs: batch_xs, ys: batch_ys&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># print(mnist.test.images, mnist.test.labels)</span></span><br><span class="line">        <span class="built_in">print</span>(compute_accuracy(mnist.test.images, mnist.test.labels))</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2018/01/08/TensorFlow-Classification%E5%88%86%E7%B1%BB%E5%AD%A6%E4%B9%A0/" data-id="cl3jmf7n100439jv3dzeecdni" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><a class="extend next" rel="next" href="/page/2/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/JavaScript/">JavaScript</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tensorflow/">Tensorflow</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/iOS/">iOS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/">未分类</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB%E6%9D%82%E8%B0%88/">生活杂谈</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E6%99%AE%E7%BA%A7%E5%88%AB/">计算机科普级别</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%B0%E5%BD%95%E9%97%AE%E9%A2%98%E5%92%8C%E5%BF%83%E5%BE%97/">记录问题和心得</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">五月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">二月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">一月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">九月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">六月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">二月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">一月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">十月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">九月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">八月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">七月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">六月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">五月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">四月 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/05/25/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2022/05/24/Hexo%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA/">Hexo快速搭建</a>
          </li>
        
          <li>
            <a href="/2018/02/13/Nginx-Mysql-PM2-NODE-GIT-HTTPS%E5%BC%80%E5%8F%91%E6%97%A5%E8%AE%B0/">Nginx+Mysql+PM2+NODE+GIT+HTTPS开发日记</a>
          </li>
        
          <li>
            <a href="/2018/01/21/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%889%EF%BC%89/">tensorflow笔记（9）</a>
          </li>
        
          <li>
            <a href="/2018/01/19/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%888%EF%BC%89/">tensorflow笔记（8）</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 [object Object]<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
  <!-- highlight.js代码高亮主题 script 引入-->
  <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- highlight.js代码高亮主题 script 引入-->
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>