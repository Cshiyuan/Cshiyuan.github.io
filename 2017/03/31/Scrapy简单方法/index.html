<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Scrapy简单方法 | C&#39;s Notebook</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="网络爬虫，是在网上进行数据抓取的程序，使用它能够抓取特定网页的HTML数据。虽然我们利用一些库开发一个爬虫程序，但是使用框架可以大大提高效率，缩短开发时间。Scrapy是一个使用Python编写的，轻量级的，简单轻巧，并且使用起来非常的方便。使用Scrapy可以很方便的完成网上数据的采集工作，它为我们完成了大量的工作，而不需要自己费大力气去开发。 首先先要回答一个问题。问：把网站装进爬虫里，总共分">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy简单方法">
<meta property="og:url" content="https://cshiyuan.github.io/2017/03/31/Scrapy%E7%AE%80%E5%8D%95%E6%96%B9%E6%B3%95/index.html">
<meta property="og:site_name" content="C&#39;s Notebook">
<meta property="og:description" content="网络爬虫，是在网上进行数据抓取的程序，使用它能够抓取特定网页的HTML数据。虽然我们利用一些库开发一个爬虫程序，但是使用框架可以大大提高效率，缩短开发时间。Scrapy是一个使用Python编写的，轻量级的，简单轻巧，并且使用起来非常的方便。使用Scrapy可以很方便的完成网上数据的采集工作，它为我们完成了大量的工作，而不需要自己费大力气去开发。 首先先要回答一个问题。问：把网站装进爬虫里，总共分">
<meta property="og:locale">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/211943L01-0.jpg">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/2119434W5-1.jpg">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/21194340K-3.jpg">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/211943GH-5.jpg">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/2119432L4-6.png">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/2119435a0-7.png">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/2119432254-8.png">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/2119431b3-9.jpg">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/211943H08-10.jpg">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/211943C96-11.jpg">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/2119432348-12.jpg">
<meta property="article:published_time" content="2017-03-31T05:21:13.000Z">
<meta property="article:modified_time" content="2022-05-23T08:43:39.079Z">
<meta property="article:author" content="shyiuanchen">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://img.xker.com/xkerfiles/allimg/1411/211943L01-0.jpg">
  
    <link rel="alternate" href="/atom.xml" title="C&#39;s Notebook" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.2.0"></head>
  <!-- highlight.js代码高亮主题 css 引入-->
  <link rel="stylesheet" href="/plugins/highlight/styles/monokai.css">
  <!-- highlight.js代码高亮主题 css 引入-->
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">C&#39;s Notebook</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://cshiyuan.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Scrapy简单方法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/03/31/Scrapy%E7%AE%80%E5%8D%95%E6%96%B9%E6%B3%95/" class="article-date">
  <time datetime="2017-03-31T05:21:13.000Z" itemprop="datePublished">2017-03-31</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Python/">Python</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Scrapy简单方法
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>网络爬虫，是在网上进行数据抓取的程序，使用它能够抓取特定网页的HTML数据。虽然我们利用一些库开发一个爬虫程序，但是使用框架可以大大提高效率，缩短开发时间。Scrapy是一个使用<a target="_blank" rel="noopener" href="http://lib.csdn.net/base/python" title="Python知识库">Python</a>编写的，轻量级的，简单轻巧，并且使用起来非常的方便。使用Scrapy可以很方便的完成网上数据的采集工作，它为我们完成了大量的工作，而不需要自己费大力气去开发。</p>
<p>首先先要回答一个问题。<br>问：把网站装进爬虫里，总共分几步？<br>答案很简单，四步：<br>新建项目 (Project)：新建一个新的爬虫项目<br>明确目标（Items）：明确你想要抓取的目标<br>制作爬虫（Spider）：制作爬虫开始爬取网页<br>存储内容（Pipeline）：设计管道存储爬取内容</p>
<p>好的，基本流程既然确定了，那接下来就一步一步的完成就可以了。</p>
<p><strong>1.新建项目（Project）</strong><br>在空目录下按住Shift键右击，选择“在此处打开命令窗口”，输入一下命令：</p>
<div class="codetitle">代码如下:</div>
<div id="code37356" class="codebody">
scrapy startproject tutorial</div>
其中，tutorial为项目名称。
可以看到将会创建一个tutorial文件夹，目录结构如下：
代码如下:

<pre><code>tutorial/
scrapy.cfg
tutorial/
__init__.py
items.py
pipelines.py
settings.py
spiders/
__init__.py
...
</code></pre>
<p>下面来简单介绍一下各个文件的作用：</p>
<ul>
<li>scrapy.cfg：项目的配置文件</li>
<li>tutorial&#x2F;：项目的Python模块，将会从这里引用代码</li>
<li>tutorial&#x2F;items.py：项目的items文件</li>
<li>tutorial&#x2F;pipelines.py：项目的pipelines文件</li>
<li>tutorial&#x2F;settings.py：项目的设置文件</li>
<li>tutorial&#x2F;spiders&#x2F;：存储爬虫的目录</li>
</ul>
<p><strong>2.明确目标（Item）</strong><br>在Scrapy中，items是用来加载抓取内容的容器，有点像Python中的Dic，也就是字典，但是提供了一些额外的保护减少错误。<br>一般来说，item可以用scrapy.item.Item类来创建，并且用scrapy.item.Field对象来定义属性（可以理解成类似于ORM的映射关系）。<br>接下来，我们开始来构建item模型（model）。<br>首先，我们想要的内容有：<br>名称（name）<br>链接（url）<br>描述（description）</p>
<p>修改tutorial目录下的items.py文件，在原本的class后面添加我们自己的class。<br>因为要抓dmoz.org网站的内容，所以我们可以将其命名为DmozItem：<br>代码如下:</p>
<pre><code># Define here the models for your scraped items
#
# See documentation in:
# http://doc.scrapy.org/en/latest/topics/items.html
from scrapy.item import Item, Field

class TutorialItem(Item):
# define the fields for your item here like:
# name = Field()
pass

class DmozItem(Item):
title = Field()
link = Field()
desc = Field()
</code></pre>
<p><strong>3.制作爬虫（Spider）</strong></p>
<p>制作爬虫，总体分两步：先爬再取。<br>也就是说，首先你要获取整个网页的所有内容，然后再取出其中对你有用的部分。<br>**3.1爬<br>**Spider是用户自己编写的类，用来从一个域（或域组）中抓取信息。<br>他们定义了用于下载的URL列表、跟踪链接的方案、解析网页内容的方式，以此来提取items。<br>要建立一个Spider，你必须用scrapy.spider.BaseSpider创建一个子类，并确定三个强制的属性：<br>name：爬虫的识别名称，必须是唯一的，在不同的爬虫中你必须定义不同的名字。<br>start_urls：爬取的URL列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些urls开始。其他子URL将会从这些起始URL中继承性生成。<br>parse()：解析的方法，调用的时候传入从每一个URL传回的Response对象作为唯一参数，负责解析并匹配抓取的数据(解析为item)，跟踪更多的URL。</p>
<p>这里可以参考宽度爬虫教程中提及的思想来帮助理解，教程传送：[<a target="_blank" rel="noopener" href="http://lib.csdn.net/base/javase" title="Java SE知识库">Java</a>] 知乎下巴第5集：使用HttpClient工具包和宽度爬虫。<br>也就是把Url存储下来并依此为起点逐步扩散开去，抓取所有符合条件的网页Url存储起来继续爬取。</p>
<p>下面我们来写第一只爬虫，命名为dmoz_spider.py，保存在tutorial\spiders目录下。<br>dmoz_spider.py代码如下：</p>
<pre><code>from scrapy.spider import Spider
class DmozSpider(Spider):
name = “dmoz”
allowed_domains = [“dmoz.org”]
start_urls = [
“http://www.dmoz.org/Computers/Programming/Languages/Python/Books/“,
“http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/”
]

def parse(self, response):
filename = response.url.split(“/”)[-2]
open(filename, ‘wb’).write(response.body)
</code></pre>
<p>allow_domains是搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页。<br>从parse函数可以看出，将链接的最后两个地址取出作为文件名进行存储。<br>然后运行一下看看，在tutorial目录下按住shift右击，在此处打开命令窗口，输入：<br>代码如下:</p>
<pre><code>scrapy crawl dmoz
</code></pre>
<p>运行结果如图：</p>
<p><img src="http://img.xker.com/xkerfiles/allimg/1411/211943L01-0.jpg"></p>
<p>报错了：<br>UnicodeDecodeError: ‘ascii’ codec can’t decode byte 0xb0 in position 1: ordinal not in range(128)<br>运行第一个Scrapy项目就报错，真是命运多舛。<br>应该是出了编码问题，谷歌了一下找到了解决方案：<br>在python的Lib\site-packages文件夹下新建一个sitecustomize.py：</p>
<div class="codetitle">代码如下:</div>
<div id="code10248" class="codebody">
import sys
sys.setdefaultencoding('gb2312')</div>
再次运行，OK，问题解决了，看一下结果：

<p><img src="http://img.xker.com/xkerfiles/allimg/1411/2119434W5-1.jpg"></p>
<p>最后一句INFO: Closing spider (finished)表明爬虫已经成功运行并且自行关闭了。<br>包含 [dmoz]的行 ，那对应着我们的爬虫运行的结果。<br>可以看到start_urls中定义的每个URL都有日志行。<br>还记得我们的start_urls吗？<br><a target="_blank" rel="noopener" href="http://www.dmoz.org/Computers/Programming/Languages/Python/Books">http://www.dmoz.org/Computers/Programming/Languages/Python/Books</a><br><a target="_blank" rel="noopener" href="http://www.dmoz.org/Computers/Programming/Languages/Python/Resources">http://www.dmoz.org/Computers/Programming/Languages/Python/Resources</a><br>因为这些URL是起始页面，所以他们没有引用(referrers)，所以在它们的每行末尾你会看到 (referer: &lt;None&gt;)。<br>在parse 方法的作用下，两个文件被创建：分别是 Books 和 Resources，这两个文件中有URL的页面内容。</p>
<p>那么在刚刚的电闪雷鸣之中到底发生了什么呢？<br>首先，Scrapy为爬虫的 start_urls属性中的每个URL创建了一个 scrapy.http.Request 对象 ，并将爬虫的parse 方法指定为回调函数。<br>然后，这些 Request被调度并执行，之后通过parse()方法返回scrapy.http.Response对象，并反馈给爬虫。</p>
<p><strong>3.2取</strong><br>爬取整个网页完毕，接下来的就是的取过程了。<br>光存储一整个网页还是不够用的。<br>在基础的爬虫里，这一步可以用正则表达式来抓。<br>在Scrapy里，使用一种叫做 XPath selectors的机制，它基于 XPath表达式。<br>如果你想了解更多selectors和其他机制你可以查阅资料：点我点我</p>
<p>这是一些XPath表达式的例子和他们的含义<br>&#x2F;html&#x2F;head&#x2F;title: 选择HTML文档&lt;head&gt;元素下面的&lt;title&gt; 标签。<br>&#x2F;html&#x2F;head&#x2F;title&#x2F;text(): 选择前面提到的&lt;title&gt; 元素下面的文本内容<br>&#x2F;&#x2F;td: 选择所有 &lt;td&gt; 元素<br>&#x2F;&#x2F;div[@class&#x3D;”mine”]: 选择所有包含 class&#x3D;”mine” 属性的div 标签元素<br>以上只是几个使用XPath的简单例子，但是实际上XPath非常强大。<br>可以参照W3C教程：点我点我。</p>
<p>为了方便使用XPaths，Scrapy提供XPathSelector 类，有两种可以选择，HtmlXPathSelector(HTML数据解析)和XmlXPathSelector(XML数据解析)。<br>必须通过一个 Response 对象对他们进行实例化操作。<br>你会发现Selector对象展示了文档的节点结构。因此，第一个实例化的selector必与根节点或者是整个目录有关 。<br>在Scrapy里面，Selectors 有四种基础的方法（点击查看API文档）：<br>xpath()：返回一系列的selectors，每一个select表示一个xpath参数表达式选择的节点<br>css()：返回一系列的selectors，每一个select表示一个css参数表达式选择的节点<br>extract()：返回一个unicode字符串，为选中的数据<br>re()：返回一串一个unicode字符串，为使用正则表达式抓取出来的内容</p>
<p>3.3xpath实验<br>下面我们在Shell里面尝试一下Selector的用法。<br>实验的网址：<a target="_blank" rel="noopener" href="http://www.dmoz.org/Computers/Programming/Languages/Python/Books/">http://www.dmoz.org/Computers/Programming/Languages/Python/Books/</a></p>
<p>熟悉完了实验的小白鼠，接下来就是用Shell爬取网页了。<br>进入到项目的顶层目录，也就是第一层tutorial文件夹下，在cmd中输入：</p>
<div class="codetitle">代码如下:</div>
<div id="code36951" class="codebody">
scrapy shell [http://www.dmoz.org/Computers/Programming/Languages/Python/Books/](http://www.dmoz.org/Computers/Programming/Languages/Python/Books/)</div>
回车后可以看到如下的内容：

<p><img src="http://img.xker.com/xkerfiles/allimg/1411/21194340K-3.jpg"></p>
<p>在Shell载入后，你将获得response回应，存储在本地变量 response中。<br>所以如果你输入response.body，你将会看到response的body部分，也就是抓取到的页面内容：</p>
<p>或者输入response.headers 来查看它的 header部分：</p>
<p><img src="http://img.xker.com/xkerfiles/allimg/1411/211943GH-5.jpg"></p>
<p>现在就像是一大堆沙子握在手里，里面藏着我们想要的金子，所以下一步，就是用筛子摇两下，把杂质出去，选出关键的内容。<br>selector就是这样一个筛子。<br>在旧的版本中，Shell实例化两种selectors，一个是解析HTML的 hxs 变量，一个是解析XML 的 xxs 变量。<br>而现在的Shell为我们准备好的selector对象，sel，可以根据返回的数据类型自动选择最佳的解析方案(XML or HTML)。<br>然后我们来捣弄一下！~<br>要彻底搞清楚这个问题，首先先要知道，抓到的页面到底是个什么样子。<br>比如，我们要抓取网页的标题，也就是&lt;title&gt;这个标签：</p>
<p><img src="http://img.xker.com/xkerfiles/allimg/1411/2119432L4-6.png"></p>
<p>可以输入：</p>
<div class="codetitle">代码如下:</div>
<div id="code56841" class="codebody">
sel.xpath('//title')</div>
结果就是：

<p><img src="http://img.xker.com/xkerfiles/allimg/1411/2119435a0-7.png"></p>
<p>这样就能把这个标签取出来了，用extract()和text()还可以进一步做处理。<br>备注：简单的罗列一下有用的xpath路径表达式：<br>表达式 描述<br>nodename 选取此节点的所有子节点。<br>&#x2F; 从根节点选取。<br>&#x2F;&#x2F; 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。<br>. 选取当前节点。<br>.. 选取当前节点的父节点。<br>@ 选取属性。<br>全部的实验结果如下，In[i]表示第i次实验的输入，Out[i]表示第i次结果的输出（建议大家参照：W3C教程）：</p>
<div class="codetitle">代码如下:</div>
<div id="code23729" class="codebody">
In [1]: sel.xpath('//title')
Out[1]: [&lt;Selector xpath='//title' data=u'&lt;title&gt;Open Directory - Computers: Progr'&gt;]

<p>In [2]: sel.xpath(‘&#x2F;&#x2F;title’).extract()<br>Out[2]: [u’&lt;title&gt;Open Directory - Computers: Programming: Languages: Python: Books&lt;&#x2F;title&gt;’]</p>
<p>In [3]: sel.xpath(‘&#x2F;&#x2F;title&#x2F;text()’)<br>Out[3]: [&lt;Selector xpath&#x3D;’&#x2F;&#x2F;title&#x2F;text()’ data&#x3D;u’Open Directory - Computers: Programming:’&gt;]</p>
<p>In [4]: sel.xpath(‘&#x2F;&#x2F;title&#x2F;text()’).extract()<br>Out[4]: [u’Open Directory - Computers: Programming: Languages: Python: Books’]</p>
<p>In [5]: sel.xpath(‘&#x2F;&#x2F;title&#x2F;text()’).re(‘(\w+):’)<br>Out[5]: [u’Computers’, u’Programming’, u’Languages’, u’Python’]</div><br>当然title这个标签对我们来说没有太多的价值，下面我们就来真正抓取一些有意义的东西。<br>使用火狐的审查元素我们可以清楚地看到，我们需要的东西如下：</p>
<p><img src="http://img.xker.com/xkerfiles/allimg/1411/2119432254-8.png"></p>
<p>我们可以用如下代码来抓取这个&lt;li&gt;标签：</p>
<div class="codetitle">代码如下:</div>
<div id="code23065" class="codebody">
sel.xpath('//ul/li')</div>
从&lt;li&gt;标签中，可以这样获取网站的描述：
<div class="codetitle">代码如下:</div>
<div id="code98498" class="codebody">
sel.xpath('//ul/li/text()').extract()</div>
可以这样获取网站的标题：
<div class="codetitle">代码如下:</div>
<div id="code72200" class="codebody">
sel.xpath('//ul/li/a/text()').extract()</div>
可以这样获取网站的超链接：
<div class="codetitle">代码如下:</div>
<div id="code24538" class="codebody">
sel.xpath(<a>'//ul/li/a/@href').extract</a>()</div>
当然，前面的这些例子是直接获取属性的方法。
我们注意到xpath返回了一个对象列表，
那么我们也可以直接调用这个列表中对象的属性挖掘更深的节点
（参考：Nesting selectors andWorking with relative XPaths in the Selectors）：
sites = sel.xpath('//ul/li')
for site in sites:
title = site.xpath('a/text()').extract()
link = site.xpath(<a>'a/@href').extract</a>()
desc = site.xpath('text()').extract()
print title, link, desc

<p>3.4xpath实战<br>我们用shell做了这么久的实战，最后我们可以把前面学习到的内容应用到dmoz_spider这个爬虫中。<br>在原爬虫的parse函数中做如下修改：</p>
<div class="codetitle">代码如下:</div>
<div id="code67523" class="codebody">
from scrapy.spider import Spider
from scrapy.selector import Selector

<p>class DmozSpider(Spider):<br>name &#x3D; “dmoz”<br>allowed_domains &#x3D; [“dmoz.org”]<br>start_urls &#x3D; [<br>“<a target="_blank" rel="noopener" href="http://www.dmoz.org/Computers/Programming/Languages/Python/Books/">http://www.dmoz.org/Computers/Programming/Languages/Python/Books/</a>“,<br>“<a target="_blank" rel="noopener" href="http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/">http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/</a>“<br>]</p>
<p>def parse(self, response):<br>sel &#x3D; Selector(response)<br>sites &#x3D; sel.xpath(‘&#x2F;&#x2F;ul&#x2F;li’)<br>for site in sites:<br>title &#x3D; site.xpath(‘a&#x2F;text()’).extract()<br>link &#x3D; site.xpath(<a>‘a&#x2F;@href’).extract</a>()<br>desc &#x3D; site.xpath(‘text()’).extract()<br>print title</div><br>注意，我们从scrapy.selector中导入了Selector类，并且实例化了一个新的Selector对象。这样我们就可以像Shell中一样操作xpath了。<br>我们来试着输入一下命令运行爬虫（在tutorial根目录里面）：</p>
<div class="codetitle">代码如下:</div>
<div id="code91685" class="codebody">
scrapy crawl dmoz</div>
运行结果如下：

<p><img src="http://img.xker.com/xkerfiles/allimg/1411/2119431b3-9.jpg"></p>
<p>果然，成功的抓到了所有的标题。但是好像不太对啊，怎么Top，Python这种导航栏也抓取出来了呢？<br>我们只需要红圈中的内容：</p>
<p><img src="http://img.xker.com/xkerfiles/allimg/1411/211943H08-10.jpg"></p>
<p>看来是我们的xpath语句有点问题，没有仅仅把我们需要的项目名称抓取出来，也抓了一些无辜的但是xpath语法相同的元素。<br>审查元素我们发现我们需要的&lt;ul&gt;具有class&#x3D;’directory-url’的属性，<br>那么只要把xpath语句改成sel.xpath(‘&#x2F;&#x2F;ul[@class&#x3D;”directory-url”]&#x2F;li’)即可<br>将xpath语句做如下调整：</p>
<div class="codetitle">代码如下:</div>
<div id="code19197" class="codebody">
from scrapy.spider import Spider
from scrapy.selector import Selector

<p>class DmozSpider(Spider):<br>name &#x3D; “dmoz”<br>allowed_domains &#x3D; [“dmoz.org”]<br>start_urls &#x3D; [<br>“<a target="_blank" rel="noopener" href="http://www.dmoz.org/Computers/Programming/Languages/Python/Books/">http://www.dmoz.org/Computers/Programming/Languages/Python/Books/</a>“,<br>“<a target="_blank" rel="noopener" href="http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/">http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/</a>“<br>]</p>
<p>def parse(self, response):<br>sel &#x3D; Selector(response)<br>sites &#x3D; sel.xpath(‘&#x2F;&#x2F;ul[@class&#x3D;”directory-url”]&#x2F;li’)<br>for site in sites:<br>title &#x3D; site.xpath(‘a&#x2F;text()’).extract()<br>link &#x3D; site.xpath(<a>‘a&#x2F;@href’).extract</a>()<br>desc &#x3D; site.xpath(‘text()’).extract()<br>print title</div><br>成功抓出了所有的标题，绝对没有滥杀无辜：</p>
<p><img src="http://img.xker.com/xkerfiles/allimg/1411/211943C96-11.jpg"></p>
<p>3.5使用Item<br>接下来我们来看一看如何使用Item。<br>前面我们说过，Item 对象是自定义的python字典，可以使用标准字典语法获取某个属性的值：</p>
<div class="codetitle">代码如下:</div>
<div id="code83545" class="codebody">
&gt;&gt;&gt; item = DmozItem()
&gt;&gt;&gt; item['title'] = 'Example title'
&gt;&gt;&gt; item['title']
'Example title'</div>
作为一只爬虫，Spiders希望能将其抓取的数据存放到Item对象中。为了返回我们抓取数据，spider的最终代码应当是这样:
代码如下:

<pre><code>from scrapy.spider import Spider
from scrapy.selector import Selector
from tutorial.items import DmozItem

class DmozSpider(Spider):
name = “dmoz”
allowed_domains = [“dmoz.org”]
start_urls = [
“http://www.dmoz.org/Computers/Programming/Languages/Python/Books/“,
“http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/”
]

def parse(self, response):
sel = Selector(response)
sites = sel.xpath(‘//ul[@class=”directory-url”]/li’)
items = []
for site in sites:
item = DmozItem()
item[‘title’] = site.xpath(‘a/text()’).extract()
item[‘link’] = site.xpath(‘a/@href’).extract()
item[‘desc’] = site.xpath(‘text()’).extract()
items.append(item)
return items
</code></pre>
<p>4.存储内容（Pipeline）<br>保存信息的最简单的方法是通过Feed exports，主要有四种：JSON，JSON lines，CSV，XML。<br>我们将结果用最常用的JSON导出，命令如下：<br>代码如下:</p>
<div id="code89953" class="codebody">
scrapy crawl dmoz -o items.json -t json</div>
-o 后面是导出文件名，-t 后面是导出类型。
然后来看一下导出的结果，用文本编辑器打开json文件即可（为了方便显示，在item中删去了除了title之外的属性）：

<p><img src="http://img.xker.com/xkerfiles/allimg/1411/2119432348-12.jpg"></p>
<p>因为这个只是一个小型的例子，所以这样简单的处理就可以了。<br>如果你想用抓取的items做更复杂的事情，你可以写一个 Item Pipeline(条目管道)。<br>这个我们以后再慢慢玩^_^</p>
<p>以上便是python爬虫框架Scrapy制作爬虫抓取网站内容的全部过程了，非常的详尽吧，希望能够对大家有所帮助，有需要的话也可以和我联系，一起进步</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://cshiyuan.github.io/2017/03/31/Scrapy%E7%AE%80%E5%8D%95%E6%96%B9%E6%B3%95/" data-id="cl3jmy3s0002xtxv3h1kt2pux" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/03/31/Scrapy%E7%88%ACGitHub%E6%97%A5%E8%AE%B0/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Scrapy爬GitHub日记
        
      </div>
    </a>
  
  
    <a href="/2017/03/30/SIP%E6%9C%BA%E5%88%B6/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">SIP机制</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/JavaScript/">JavaScript</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tensorflow/">Tensorflow</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/iOS/">iOS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/">未分类</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB%E6%9D%82%E8%B0%88/">生活杂谈</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E6%99%AE%E7%BA%A7%E5%88%AB/">计算机科普级别</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%B0%E5%BD%95%E9%97%AE%E9%A2%98%E5%92%8C%E5%BF%83%E5%BE%97/">记录问题和心得</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">May 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">June 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">April 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/05/25/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2022/05/24/Hexo%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA/">Hexo快速搭建</a>
          </li>
        
          <li>
            <a href="/2018/02/13/Nginx-Mysql-PM2-NODE-GIT-HTTPS%E5%BC%80%E5%8F%91%E6%97%A5%E8%AE%B0/">Nginx+Mysql+PM2+NODE+GIT+HTTPS开发日记</a>
          </li>
        
          <li>
            <a href="/2018/01/21/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%889%EF%BC%89/">tensorflow笔记（9）</a>
          </li>
        
          <li>
            <a href="/2018/01/19/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%888%EF%BC%89/">tensorflow笔记（8）</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 shyiuanchen<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
  <!-- highlight.js代码高亮主题 script 引入-->
  <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- highlight.js代码高亮主题 script 引入-->
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>