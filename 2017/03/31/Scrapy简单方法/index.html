<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Scrapy简单方法 | CShyiuan博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="网络爬虫，是在网上进行数据抓取的程序，使用它能够抓取特定网页的HTML数据。虽然我们利用一些库开发一个爬虫程序，但是使用框架可以大大提高效率，缩短开发时间。Scrapy是一个使用Python编写的，轻量级的，简单轻巧，并且使用起来非常的方便。使用Scrapy可以很方便的完成网上数据的采集工作，它为我们完成了大量的工作，而不需要自己费大力气去开发。 首先先要回答一个问题。 问：把网站装进爬虫里，总共">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy简单方法">
<meta property="og:url" content="http://example.com/2017/03/31/Scrapy%E7%AE%80%E5%8D%95%E6%96%B9%E6%B3%95/index.html">
<meta property="og:site_name" content="CShyiuan博客">
<meta property="og:description" content="网络爬虫，是在网上进行数据抓取的程序，使用它能够抓取特定网页的HTML数据。虽然我们利用一些库开发一个爬虫程序，但是使用框架可以大大提高效率，缩短开发时间。Scrapy是一个使用Python编写的，轻量级的，简单轻巧，并且使用起来非常的方便。使用Scrapy可以很方便的完成网上数据的采集工作，它为我们完成了大量的工作，而不需要自己费大力气去开发。 首先先要回答一个问题。 问：把网站装进爬虫里，总共">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/211943L01-0.jpg">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/2119434W5-1.jpg">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/21194340K-3.jpg">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/211943GH-5.jpg">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/2119432L4-6.png">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/2119435a0-7.png">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/2119432254-8.png">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/2119431b3-9.jpg">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/211943H08-10.jpg">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/211943C96-11.jpg">
<meta property="og:image" content="http://img.xker.com/xkerfiles/allimg/1411/2119432348-12.jpg">
<meta property="article:published_time" content="2017-03-31T05:21:13.000Z">
<meta property="article:modified_time" content="2022-05-23T08:43:39.079Z">
<meta property="article:author" content="[object Object]">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://img.xker.com/xkerfiles/allimg/1411/211943L01-0.jpg">
  
    <link rel="alternate" href="/atom.xml" title="CShyiuan博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.2.0"></head>
  <!-- highlight.js代码高亮主题 css 引入-->
  <link rel="stylesheet" href="/plugins/highlight/styles/monokai.css">
  <!-- highlight.js代码高亮主题 css 引入-->
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">CShyiuan博客</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Scrapy简单方法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/03/31/Scrapy%E7%AE%80%E5%8D%95%E6%96%B9%E6%B3%95/" class="article-date">
  <time datetime="2017-03-31T05:21:13.000Z" itemprop="datePublished">2017-03-31</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Python/">Python</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Scrapy简单方法
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>网络爬虫，是在网上进行数据抓取的程序，使用它能够抓取特定网页的HTML数据。虽然我们利用一些库开发一个爬虫程序，但是使用框架可以大大提高效率，缩短开发时间。Scrapy是一个使用<a target="_blank" rel="noopener" href="http://lib.csdn.net/base/python" title="Python知识库">Python</a>编写的，轻量级的，简单轻巧，并且使用起来非常的方便。使用Scrapy可以很方便的完成网上数据的采集工作，它为我们完成了大量的工作，而不需要自己费大力气去开发。</p>
<p>首先先要回答一个问题。<br>
问：把网站装进爬虫里，总共分几步？<br>
答案很简单，四步：<br>
新建项目 (Project)：新建一个新的爬虫项目<br>
明确目标（Items）：明确你想要抓取的目标<br>
制作爬虫（Spider）：制作爬虫开始爬取网页<br>
存储内容（Pipeline）：设计管道存储爬取内容</p>
<p>好的，基本流程既然确定了，那接下来就一步一步的完成就可以了。</p>
<p><strong>1.新建项目（Project）</strong><br>
在空目录下按住Shift键右击，选择“在此处打开命令窗口”，输入一下命令：</p>
<div class="codetitle">代码如下:</div>
<div id="code37356" class="codebody">
scrapy startproject tutorial</div>
其中，tutorial为项目名称。
可以看到将会创建一个tutorial文件夹，目录结构如下：
代码如下:
<p class="code-caption" data-lang="" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tutorial/</span><br><span class="line">scrapy.cfg</span><br><span class="line">tutorial/</span><br><span class="line">__init__.py</span><br><span class="line">items.py</span><br><span class="line">pipelines.py</span><br><span class="line">settings.py</span><br><span class="line">spiders/</span><br><span class="line">__init__.py</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>下面来简单介绍一下各个文件的作用：</p>
<ul>
<li>scrapy.cfg：项目的配置文件</li>
<li>tutorial/：项目的Python模块，将会从这里引用代码</li>
<li>tutorial/items.py：项目的items文件</li>
<li>tutorial/pipelines.py：项目的pipelines文件</li>
<li>tutorial/settings.py：项目的设置文件</li>
<li>tutorial/spiders/：存储爬虫的目录</li>
</ul>
<p><strong>2.明确目标（Item）</strong><br>
在Scrapy中，items是用来加载抓取内容的容器，有点像Python中的Dic，也就是字典，但是提供了一些额外的保护减少错误。<br>
一般来说，item可以用scrapy.item.Item类来创建，并且用scrapy.item.Field对象来定义属性（可以理解成类似于ORM的映射关系）。<br>
接下来，我们开始来构建item模型（model）。<br>
首先，我们想要的内容有：<br>
名称（name）<br>
链接（url）<br>
描述（description）</p>
<p>修改tutorial目录下的items.py文件，在原本的class后面添加我们自己的class。<br>
因为要抓dmoz.org网站的内容，所以我们可以将其命名为DmozItem：<br>
代码如下:</p>
<p class="code-caption" data-lang="" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># Define here the models for your scraped items</span><br><span class="line">#</span><br><span class="line"># See documentation in:</span><br><span class="line"># http://doc.scrapy.org/en/latest/topics/items.html</span><br><span class="line">from scrapy.item import Item, Field</span><br><span class="line"></span><br><span class="line">class TutorialItem(Item):</span><br><span class="line"># define the fields for your item here like:</span><br><span class="line"># name = Field()</span><br><span class="line">pass</span><br><span class="line"></span><br><span class="line">class DmozItem(Item):</span><br><span class="line">title = Field()</span><br><span class="line">link = Field()</span><br><span class="line">desc = Field()</span><br></pre></td></tr></table></figure>
<p><strong>3.制作爬虫（Spider）</strong></p>
<p>制作爬虫，总体分两步：先爬再取。<br>
也就是说，首先你要获取整个网页的所有内容，然后再取出其中对你有用的部分。<br>
**3.1爬<br>
**Spider是用户自己编写的类，用来从一个域（或域组）中抓取信息。<br>
他们定义了用于下载的URL列表、跟踪链接的方案、解析网页内容的方式，以此来提取items。<br>
要建立一个Spider，你必须用scrapy.spider.BaseSpider创建一个子类，并确定三个强制的属性：<br>
name：爬虫的识别名称，必须是唯一的，在不同的爬虫中你必须定义不同的名字。<br>
start_urls：爬取的URL列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些urls开始。其他子URL将会从这些起始URL中继承性生成。<br>
parse()：解析的方法，调用的时候传入从每一个URL传回的Response对象作为唯一参数，负责解析并匹配抓取的数据(解析为item)，跟踪更多的URL。</p>
<p>这里可以参考宽度爬虫教程中提及的思想来帮助理解，教程传送：[<a target="_blank" rel="noopener" href="http://lib.csdn.net/base/javase" title="Java SE知识库">Java</a>] 知乎下巴第5集：使用HttpClient工具包和宽度爬虫。<br>
也就是把Url存储下来并依此为起点逐步扩散开去，抓取所有符合条件的网页Url存储起来继续爬取。</p>
<p>下面我们来写第一只爬虫，命名为dmoz_spider.py，保存在tutorial\spiders目录下。<br>
dmoz_spider.py代码如下：</p>
<p class="code-caption" data-lang="" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.spider import Spider</span><br><span class="line">class DmozSpider(Spider):</span><br><span class="line">name = “dmoz”</span><br><span class="line">allowed_domains = [“dmoz.org”]</span><br><span class="line">start_urls = [</span><br><span class="line">“http://www.dmoz.org/Computers/Programming/Languages/Python/Books/“,</span><br><span class="line">“http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/”</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">def parse(self, response):</span><br><span class="line">filename = response.url.split(“/”)[-2]</span><br><span class="line">open(filename, ‘wb’).write(response.body)</span><br></pre></td></tr></table></figure>
<p>allow_domains是搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页。<br>
从parse函数可以看出，将链接的最后两个地址取出作为文件名进行存储。<br>
然后运行一下看看，在tutorial目录下按住shift右击，在此处打开命令窗口，输入：<br>
代码如下:</p>
<p class="code-caption" data-lang="" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl dmoz</span><br></pre></td></tr></table></figure>
<p>运行结果如图：</p>
<p><img src="http://img.xker.com/xkerfiles/allimg/1411/211943L01-0.jpg" alt=""></p>
<p>报错了：<br>
UnicodeDecodeError: ‘ascii’ codec can’t decode byte 0xb0 in position 1: ordinal not in range(128)<br>
运行第一个Scrapy项目就报错，真是命运多舛。<br>
应该是出了编码问题，谷歌了一下找到了解决方案：<br>
在python的Lib\<a target="_blank" rel="noopener" href="http://xn--site-packagessitecustomize-sk15b1i44a18o106g1qvbcg7b6eb.py">site-packages文件夹下新建一个sitecustomize.py</a>：</p>
<div class="codetitle">代码如下:</div>
<div id="code10248" class="codebody">
import sys
sys.setdefaultencoding('gb2312')</div>
再次运行，OK，问题解决了，看一下结果：
<p><img src="http://img.xker.com/xkerfiles/allimg/1411/2119434W5-1.jpg" alt=""></p>
<p>最后一句INFO: Closing spider (finished)表明爬虫已经成功运行并且自行关闭了。<br>
包含 [dmoz]的行 ，那对应着我们的爬虫运行的结果。<br>
可以看到start_urls中定义的每个URL都有日志行。<br>
还记得我们的start_urls吗？<br>
<a target="_blank" rel="noopener" href="http://www.dmoz.org/Computers/Programming/Languages/Python/Books">http://www.dmoz.org/Computers/Programming/Languages/Python/Books</a><br>
<a target="_blank" rel="noopener" href="http://www.dmoz.org/Computers/Programming/Languages/Python/Resources">http://www.dmoz.org/Computers/Programming/Languages/Python/Resources</a><br>
因为这些URL是起始页面，所以他们没有引用(referrers)，所以在它们的每行末尾你会看到 (referer: &lt;None&gt;)。<br>
在parse 方法的作用下，两个文件被创建：分别是 Books 和 Resources，这两个文件中有URL的页面内容。</p>
<p>那么在刚刚的电闪雷鸣之中到底发生了什么呢？<br>
首先，Scrapy为爬虫的 start_urls属性中的每个URL创建了一个 scrapy.http.Request 对象 ，并将爬虫的parse 方法指定为回调函数。<br>
然后，这些 Request被调度并执行，之后通过parse()方法返回scrapy.http.Response对象，并反馈给爬虫。</p>
<p><strong>3.2取</strong><br>
爬取整个网页完毕，接下来的就是的取过程了。<br>
光存储一整个网页还是不够用的。<br>
在基础的爬虫里，这一步可以用正则表达式来抓。<br>
在Scrapy里，使用一种叫做 XPath selectors的机制，它基于 XPath表达式。<br>
如果你想了解更多selectors和其他机制你可以查阅资料：点我点我</p>
<p>这是一些XPath表达式的例子和他们的含义<br>
/html/head/title: 选择HTML文档&lt;head&gt;元素下面的&lt;title&gt; 标签。<br>
/html/head/title/text(): 选择前面提到的&lt;title&gt; 元素下面的文本内容<br>
//td: 选择所有 &lt;td&gt; 元素<br>
//div[@class=“mine”]: 选择所有包含 class=“mine” 属性的div 标签元素<br>
以上只是几个使用XPath的简单例子，但是实际上XPath非常强大。<br>
可以参照W3C教程：点我点我。</p>
<p>为了方便使用XPaths，Scrapy提供XPathSelector 类，有两种可以选择，HtmlXPathSelector(HTML数据解析)和XmlXPathSelector(XML数据解析)。<br>
必须通过一个 Response 对象对他们进行实例化操作。<br>
你会发现Selector对象展示了文档的节点结构。因此，第一个实例化的selector必与根节点或者是整个目录有关 。<br>
在Scrapy里面，Selectors 有四种基础的方法（点击查看API文档）：<br>
xpath()：返回一系列的selectors，每一个select表示一个xpath参数表达式选择的节点<br>
css()：返回一系列的selectors，每一个select表示一个css参数表达式选择的节点<br>
extract()：返回一个unicode字符串，为选中的数据<br>
re()：返回一串一个unicode字符串，为使用正则表达式抓取出来的内容</p>
<p>3.3xpath实验<br>
下面我们在Shell里面尝试一下Selector的用法。<br>
实验的网址：<a target="_blank" rel="noopener" href="http://www.dmoz.org/Computers/Programming/Languages/Python/Books/">http://www.dmoz.org/Computers/Programming/Languages/Python/Books/</a></p>
<p>熟悉完了实验的小白鼠，接下来就是用Shell爬取网页了。<br>
进入到项目的顶层目录，也就是第一层tutorial文件夹下，在cmd中输入：</p>
<div class="codetitle">代码如下:</div>
<div id="code36951" class="codebody">
scrapy shell [http://www.dmoz.org/Computers/Programming/Languages/Python/Books/](http://www.dmoz.org/Computers/Programming/Languages/Python/Books/)</div>
回车后可以看到如下的内容：
<p><img src="http://img.xker.com/xkerfiles/allimg/1411/21194340K-3.jpg" alt=""></p>
<p>在Shell载入后，你将获得response回应，存储在本地变量 response中。<br>
所以如果你输入response.body，你将会看到response的body部分，也就是抓取到的页面内容：</p>
<p>或者输入response.headers 来查看它的 header部分：</p>
<p><img src="http://img.xker.com/xkerfiles/allimg/1411/211943GH-5.jpg" alt=""></p>
<p>现在就像是一大堆沙子握在手里，里面藏着我们想要的金子，所以下一步，就是用筛子摇两下，把杂质出去，选出关键的内容。<br>
selector就是这样一个筛子。<br>
在旧的版本中，Shell实例化两种selectors，一个是解析HTML的 hxs 变量，一个是解析XML 的 xxs 变量。<br>
而现在的Shell为我们准备好的selector对象，sel，可以根据返回的数据类型自动选择最佳的解析方案(XML or HTML)。<br>
然后我们来捣弄一下！~<br>
要彻底搞清楚这个问题，首先先要知道，抓到的页面到底是个什么样子。<br>
比如，我们要抓取网页的标题，也就是&lt;title&gt;这个标签：</p>
<p><img src="http://img.xker.com/xkerfiles/allimg/1411/2119432L4-6.png" alt=""></p>
<p>可以输入：</p>
<div class="codetitle">代码如下:</div>
<div id="code56841" class="codebody">
sel.xpath('//title')</div>
结果就是：
<p><img src="http://img.xker.com/xkerfiles/allimg/1411/2119435a0-7.png" alt=""></p>
<p>这样就能把这个标签取出来了，用extract()和text()还可以进一步做处理。<br>
备注：简单的罗列一下有用的xpath路径表达式：<br>
表达式 描述<br>
nodename 选取此节点的所有子节点。<br>
/ 从根节点选取。<br>
// 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。<br>
. 选取当前节点。<br>
… 选取当前节点的父节点。<br>
@ 选取属性。<br>
全部的实验结果如下，In[i]表示第i次实验的输入，Out[i]表示第i次结果的输出（建议大家参照：W3C教程）：</p>
<div class="codetitle">代码如下:</div>
<div id="code23729" class="codebody">
In [1]: sel.xpath('//title')
Out[1]: [&lt;Selector xpath='//title' data=u'&lt;title&gt;Open Directory - Computers: Progr'&gt;]
<p>In [2]: sel.xpath(‘//title’).extract()<br>
Out[2]: [u’&lt;title&gt;Open Directory - Computers: Programming: Languages: Python: Books&lt;/title&gt;']</p>
<p>In [3]: sel.xpath(‘//title/text()’)<br>
Out[3]: [&lt;Selector xpath=‘//title/text()’ data=u’Open Directory - Computers: Programming:'&gt;]</p>
<p>In [4]: sel.xpath(‘//title/text()’).extract()<br>
Out[4]: [u’Open Directory - Computers: Programming: Languages: Python: Books’]</p>
<p>In [5]: sel.xpath(‘//title/text()’).re(‘(\w+):’)<br>
Out[5]: [u’Computers’, u’Programming’, u’Languages’, u’Python’]</div><br>
当然title这个标签对我们来说没有太多的价值，下面我们就来真正抓取一些有意义的东西。<br>
使用火狐的审查元素我们可以清楚地看到，我们需要的东西如下：</p>
<p><img src="http://img.xker.com/xkerfiles/allimg/1411/2119432254-8.png" alt=""></p>
<p>我们可以用如下代码来抓取这个&lt;li&gt;标签：</p>
<div class="codetitle">代码如下:</div>
<div id="code23065" class="codebody">
sel.xpath('//ul/li')</div>
从&lt;li&gt;标签中，可以这样获取网站的描述：
<div class="codetitle">代码如下:</div>
<div id="code98498" class="codebody">
sel.xpath('//ul/li/text()').extract()</div>
可以这样获取网站的标题：
<div class="codetitle">代码如下:</div>
<div id="code72200" class="codebody">
sel.xpath('//ul/li/a/text()').extract()</div>
可以这样获取网站的超链接：
<div class="codetitle">代码如下:</div>
<div id="code24538" class="codebody">
sel.xpath(<a>'//ul/li/a/@href').extract</a>()</div>
当然，前面的这些例子是直接获取属性的方法。
我们注意到xpath返回了一个对象列表，
那么我们也可以直接调用这个列表中对象的属性挖掘更深的节点
（参考：Nesting selectors andWorking with relative XPaths in the Selectors）：
sites = sel.xpath('//ul/li')
for site in sites:
title = site.xpath('a/text()').extract()
link = site.xpath(<a>'a/@href').extract</a>()
desc = site.xpath('text()').extract()
print title, link, desc
<p>3.4xpath实战<br>
我们用shell做了这么久的实战，最后我们可以把前面学习到的内容应用到dmoz_spider这个爬虫中。<br>
在原爬虫的parse函数中做如下修改：</p>
<div class="codetitle">代码如下:</div>
<div id="code67523" class="codebody">
from scrapy.spider import Spider
from scrapy.selector import Selector
<p>class DmozSpider(Spider):<br>
name = “dmoz”<br>
allowed_domains = [“<a target="_blank" rel="noopener" href="http://dmoz.org">dmoz.org</a>”]<br>
start_urls = [<br>
“<a target="_blank" rel="noopener" href="http://www.dmoz.org/Computers/Programming/Languages/Python/Books/">http://www.dmoz.org/Computers/Programming/Languages/Python/Books/</a>”,<br>
“<a target="_blank" rel="noopener" href="http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/">http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/</a>”<br>
]</p>
<p>def parse(self, response):<br>
sel = Selector(response)<br>
sites = sel.xpath(‘//ul/li’)<br>
for site in sites:<br>
title = site.xpath(‘a/text()’).extract()<br>
link = site.xpath(<a>‘a/@href’).extract</a>()<br>
desc = site.xpath(‘text()’).extract()<br>
print title</div><br>
注意，我们从scrapy.selector中导入了Selector类，并且实例化了一个新的Selector对象。这样我们就可以像Shell中一样操作xpath了。<br>
我们来试着输入一下命令运行爬虫（在tutorial根目录里面）：</p>
<div class="codetitle">代码如下:</div>
<div id="code91685" class="codebody">
scrapy crawl dmoz</div>
运行结果如下：
<p><img src="http://img.xker.com/xkerfiles/allimg/1411/2119431b3-9.jpg" alt=""></p>
<p>果然，成功的抓到了所有的标题。但是好像不太对啊，怎么Top，Python这种导航栏也抓取出来了呢？<br>
我们只需要红圈中的内容：</p>
<p><img src="http://img.xker.com/xkerfiles/allimg/1411/211943H08-10.jpg" alt=""></p>
<p>看来是我们的xpath语句有点问题，没有仅仅把我们需要的项目名称抓取出来，也抓了一些无辜的但是xpath语法相同的元素。<br>
审查元素我们发现我们需要的&lt;ul&gt;具有class=‘directory-url’的属性，<br>
那么只要把xpath语句改成sel.xpath(’//ul[@class=“directory-url”]/li’)即可<br>
将xpath语句做如下调整：</p>
<div class="codetitle">代码如下:</div>
<div id="code19197" class="codebody">
from scrapy.spider import Spider
from scrapy.selector import Selector
<p>class DmozSpider(Spider):<br>
name = “dmoz”<br>
allowed_domains = [“<a target="_blank" rel="noopener" href="http://dmoz.org">dmoz.org</a>”]<br>
start_urls = [<br>
“<a target="_blank" rel="noopener" href="http://www.dmoz.org/Computers/Programming/Languages/Python/Books/">http://www.dmoz.org/Computers/Programming/Languages/Python/Books/</a>”,<br>
“<a target="_blank" rel="noopener" href="http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/">http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/</a>”<br>
]</p>
<p>def parse(self, response):<br>
sel = Selector(response)<br>
sites = sel.xpath(‘//ul[@class=“directory-url”]/li’)<br>
for site in sites:<br>
title = site.xpath(‘a/text()’).extract()<br>
link = site.xpath(<a>‘a/@href’).extract</a>()<br>
desc = site.xpath(‘text()’).extract()<br>
print title</div><br>
成功抓出了所有的标题，绝对没有滥杀无辜：</p>
<p><img src="http://img.xker.com/xkerfiles/allimg/1411/211943C96-11.jpg" alt=""></p>
<p>3.5使用Item<br>
接下来我们来看一看如何使用Item。<br>
前面我们说过，Item 对象是自定义的python字典，可以使用标准字典语法获取某个属性的值：</p>
<div class="codetitle">代码如下:</div>
<div id="code83545" class="codebody">
&gt;&gt;&gt; item = DmozItem()
&gt;&gt;&gt; item['title'] = 'Example title'
&gt;&gt;&gt; item['title']
'Example title'</div>
作为一只爬虫，Spiders希望能将其抓取的数据存放到Item对象中。为了返回我们抓取数据，spider的最终代码应当是这样:
代码如下:
<p class="code-caption" data-lang="" data-line_number="frontend" data-trim_indent="backend" data-label_position="outer" data-labels_left="Code" data-labels_right=":" data-labels_copy="Copy Code"><span class="code-caption-label"></span><a class="code-caption-copy">Copy Code</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.spider import Spider</span><br><span class="line">from scrapy.selector import Selector</span><br><span class="line">from tutorial.items import DmozItem</span><br><span class="line"></span><br><span class="line">class DmozSpider(Spider):</span><br><span class="line">name = “dmoz”</span><br><span class="line">allowed_domains = [“dmoz.org”]</span><br><span class="line">start_urls = [</span><br><span class="line">“http://www.dmoz.org/Computers/Programming/Languages/Python/Books/“,</span><br><span class="line">“http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/”</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">def parse(self, response):</span><br><span class="line">sel = Selector(response)</span><br><span class="line">sites = sel.xpath(‘//ul[@class=”directory-url”]/li’)</span><br><span class="line">items = []</span><br><span class="line">for site in sites:</span><br><span class="line">item = DmozItem()</span><br><span class="line">item[‘title’] = site.xpath(‘a/text()’).extract()</span><br><span class="line">item[‘link’] = site.xpath(‘a/@href’).extract()</span><br><span class="line">item[‘desc’] = site.xpath(‘text()’).extract()</span><br><span class="line">items.append(item)</span><br><span class="line">return items</span><br></pre></td></tr></table></figure>
<p>4.存储内容（Pipeline）<br>
保存信息的最简单的方法是通过Feed exports，主要有四种：JSON，JSON lines，CSV，XML。<br>
我们将结果用最常用的JSON导出，命令如下：<br>
代码如下:</p>
<div id="code89953" class="codebody">
scrapy crawl dmoz -o items.json -t json</div>
-o 后面是导出文件名，-t 后面是导出类型。
然后来看一下导出的结果，用文本编辑器打开json文件即可（为了方便显示，在item中删去了除了title之外的属性）：
<p><img src="http://img.xker.com/xkerfiles/allimg/1411/2119432348-12.jpg" alt=""></p>
<p>因为这个只是一个小型的例子，所以这样简单的处理就可以了。<br>
如果你想用抓取的items做更复杂的事情，你可以写一个 Item Pipeline(条目管道)。<br>
这个我们以后再慢慢玩^_^</p>
<p>以上便是python爬虫框架Scrapy制作爬虫抓取网站内容的全部过程了，非常的详尽吧，希望能够对大家有所帮助，有需要的话也可以和我联系，一起进步</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2017/03/31/Scrapy%E7%AE%80%E5%8D%95%E6%96%B9%E6%B3%95/" data-id="cl3jmf7my003r9jv38ou33isj" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/03/31/Scrapy%E7%88%ACGitHub%E6%97%A5%E8%AE%B0/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Scrapy爬GitHub日记
        
      </div>
    </a>
  
  
    <a href="/2017/03/30/SIP%E6%9C%BA%E5%88%B6/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">SIP机制</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/JavaScript/">JavaScript</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tensorflow/">Tensorflow</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/iOS/">iOS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/">未分类</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB%E6%9D%82%E8%B0%88/">生活杂谈</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E6%99%AE%E7%BA%A7%E5%88%AB/">计算机科普级别</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%B0%E5%BD%95%E9%97%AE%E9%A2%98%E5%92%8C%E5%BF%83%E5%BE%97/">记录问题和心得</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">五月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">二月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">一月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">九月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">六月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">二月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">一月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">十月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">九月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">八月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">七月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">六月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">五月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">四月 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/05/25/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2022/05/24/Hexo%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA/">Hexo快速搭建</a>
          </li>
        
          <li>
            <a href="/2018/02/13/Nginx-Mysql-PM2-NODE-GIT-HTTPS%E5%BC%80%E5%8F%91%E6%97%A5%E8%AE%B0/">Nginx+Mysql+PM2+NODE+GIT+HTTPS开发日记</a>
          </li>
        
          <li>
            <a href="/2018/01/21/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%889%EF%BC%89/">tensorflow笔记（9）</a>
          </li>
        
          <li>
            <a href="/2018/01/19/tensorflow%E7%AC%94%E8%AE%B0%EF%BC%888%EF%BC%89/">tensorflow笔记（8）</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 [object Object]<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
  <!-- highlight.js代码高亮主题 script 引入-->
  <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- highlight.js代码高亮主题 script 引入-->
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>